{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "101\n",
      "24\n",
      "24101\n"
     ]
    }
   ],
   "source": [
    "ind = '24'\n",
    "print(ind)\n",
    "\n",
    "num1 = '101'\n",
    "print(num1)\n",
    "\n",
    "# ind = ind.zfill(3)\n",
    "print(ind)\n",
    "\n",
    "unique_id2 = ind + num1\n",
    "print(unique_id2.zfill(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split dataset to train/val/test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AisazuNihaIrarenai\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "1 AkkeraKanjinchou\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "2 Akuhamu\n",
      "train file count: 60\n",
      "val file count: 12\n",
      "test file count: 12\n",
      "\n",
      "3 AosugiruHaru\n",
      "train file count: 78\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "4 AppareKappore\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "5 Arisa\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "6 ARMS\n",
      "train file count: 60\n",
      "val file count: 12\n",
      "test file count: 12\n",
      "\n",
      "7 BakuretsuKungFuGirl\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "8 Belmondo\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "9 BEMADER_P\n",
      "train file count: 85\n",
      "val file count: 17\n",
      "test file count: 17\n",
      "\n",
      "10 BokuHaSitatakaKun\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "11 BurariTessenTorimonocho\n",
      "train file count: 84\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "12 ByebyeC-BOY\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "13 Count3DeKimeteAgeru\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "14 DollGun\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "15 Donburakokko\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "16 DualJustice\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "17 EienNoWith\n",
      "train file count: 93\n",
      "val file count: 18\n",
      "test file count: 18\n",
      "\n",
      "18 EvaLady\n",
      "train file count: 81\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "19 EverydayOsakanaChan\n",
      "train file count: 64\n",
      "val file count: 12\n",
      "test file count: 12\n",
      "\n",
      "20 GakuenNoise\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "21 GarakutayaManta\n",
      "train file count: 77\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "22 GinNoChimera\n",
      "train file count: 77\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "23 GOOD_KISS_Ver2\n",
      "train file count: 71\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "24 Hamlet\n",
      "train file count: 138\n",
      "val file count: 27\n",
      "test file count: 27\n",
      "\n",
      "25 HanzaiKousyouninMinegishiEitarou\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "26 HaruichibanNoFukukoro\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "27 HarukaRefrain\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "28 HealingPlanet\n",
      "train file count: 71\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "29 HeiseiJimen\n",
      "train file count: 83\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "30 HighschoolKimengumi_vol01\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "31 HighschoolKimengumi_vol20\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "32 HinagikuKenzan\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "33 HisokaReturns\n",
      "train file count: 67\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "34 JangiriPonpon\n",
      "train file count: 82\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "35 JijiBabaFight\n",
      "train file count: 54\n",
      "val file count: 10\n",
      "test file count: 10\n",
      "\n",
      "36 Joouari\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "37 Jyovolley\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "38 KarappoHighschool\n",
      "train file count: 90\n",
      "val file count: 18\n",
      "test file count: 18\n",
      "\n",
      "39 KimiHaBokuNoTaiyouDa\n",
      "train file count: 83\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "40 KoukouNoHitotachi\n",
      "train file count: 50\n",
      "val file count: 10\n",
      "test file count: 10\n",
      "\n",
      "41 KuroidoGanka\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "42 KyokugenCyclone\n",
      "train file count: 43\n",
      "val file count: 8\n",
      "test file count: 8\n",
      "\n",
      "43 LancelotFullThrottle\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "44 LoveHina_vol01\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "45 LoveHina_vol14\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "46 MadouTaiga\n",
      "train file count: 63\n",
      "val file count: 12\n",
      "test file count: 12\n",
      "\n",
      "47 MAD_STONE\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "48 MagicianLoad\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "49 MagicStarGakuin\n",
      "train file count: 61\n",
      "val file count: 12\n",
      "test file count: 12\n",
      "\n",
      "50 MariaSamaNihaNaisyo\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "51 MayaNoAkaiKutsu\n",
      "train file count: 67\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "52 MemorySeijin\n",
      "train file count: 76\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "53 MeteoSanStrikeDesu\n",
      "train file count: 85\n",
      "val file count: 17\n",
      "test file count: 17\n",
      "\n",
      "54 MiraiSan\n",
      "train file count: 80\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "55 MisutenaideDaisy\n",
      "train file count: 83\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "56 MoeruOnisan_vol01\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "57 MoeruOnisan_vol19\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "58 MomoyamaHaikagura\n",
      "train file count: 38\n",
      "val file count: 7\n",
      "test file count: 7\n",
      "\n",
      "59 MukoukizuNoChonbo\n",
      "train file count: 47\n",
      "val file count: 9\n",
      "test file count: 9\n",
      "\n",
      "60 MutekiBoukenSyakuma\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "61 Nekodama\n",
      "train file count: 87\n",
      "val file count: 17\n",
      "test file count: 17\n",
      "\n",
      "62 NichijouSoup\n",
      "train file count: 76\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "63 Ningyoushi\n",
      "train file count: 71\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "64 OhWareraRettouSeitokai\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "65 OL_Lunch\n",
      "train file count: 50\n",
      "val file count: 10\n",
      "test file count: 10\n",
      "\n",
      "66 ParaisoRoad\n",
      "train file count: 78\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "67 PikaruGenkiDesu\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "68 PLANET7\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "69 PlatinumJungle\n",
      "train file count: 64\n",
      "val file count: 12\n",
      "test file count: 12\n",
      "\n",
      "70 PrayerHaNemurenai\n",
      "train file count: 73\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "71 PrismHeart\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "72 PsychoStaff\n",
      "train file count: 68\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "73 Raphael\n",
      "train file count: 87\n",
      "val file count: 17\n",
      "test file count: 17\n",
      "\n",
      "74 ReveryEarth\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "75 RinToSiteSippuNoNaka\n",
      "train file count: 114\n",
      "val file count: 22\n",
      "test file count: 22\n",
      "\n",
      "76 RisingGirl\n",
      "train file count: 90\n",
      "val file count: 18\n",
      "test file count: 18\n",
      "\n",
      "77 Saisoku\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "78 SaladDays_vol01\n",
      "train file count: 68\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "79 SaladDays_vol18\n",
      "train file count: 71\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "80 SamayoeruSyonenNiJunaiWo\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "81 SeisinkiVulnus\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 15\n",
      "\n",
      "82 ShimatteIkouze_vol01\n",
      "train file count: 81\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "83 ShimatteIkouze_vol26\n",
      "train file count: 84\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "84 SonokiDeABC\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "85 SyabondamaKieta\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "86 TaiyouNiSmash\n",
      "train file count: 68\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "87 TapkunNoTanteisitsu\n",
      "train file count: 80\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "88 TasogareTsushin\n",
      "train file count: 73\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "89 TennenSenshiG\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "90 TensiNoHaneToAkumaNoShippo\n",
      "train file count: 68\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "91 TetsuSan\n",
      "train file count: 30\n",
      "val file count: 6\n",
      "test file count: 6\n",
      "\n",
      "92 That'sIzumiko\n",
      "train file count: 95\n",
      "val file count: 19\n",
      "test file count: 19\n",
      "\n",
      "93 TotteokiNoABC\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "94 ToutaMairimasu\n",
      "train file count: 81\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "95 TouyouKidan\n",
      "train file count: 99\n",
      "val file count: 19\n",
      "test file count: 19\n",
      "\n",
      "96 TsubasaNoKioku\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "97 UchiNoNyan'sDiary\n",
      "train file count: 61\n",
      "val file count: 12\n",
      "test file count: 12\n",
      "\n",
      "98 UchuKigekiM774\n",
      "train file count: 62\n",
      "val file count: 12\n",
      "test file count: 12\n",
      "\n",
      "99 UltraEleven\n",
      "train file count: 84\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "100 UnbalanceTokyo\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 13\n",
      "\n",
      "101 WarewareHaOniDearu\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "102 YamatoNoHane\n",
      "train file count: 85\n",
      "val file count: 17\n",
      "test file count: 17\n",
      "\n",
      "103 YasasiiAkuma\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "104 YouchienBoueigumi\n",
      "train file count: 20\n",
      "val file count: 4\n",
      "test file count: 4\n",
      "\n",
      "105 YoumaKourin\n",
      "train file count: 80\n",
      "val file count: 16\n",
      "test file count: 16\n",
      "\n",
      "106 YukiNoFuruMachi\n",
      "train file count: 71\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "107 YumeiroCooking\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n",
      "108 YumeNoKayoiji\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(image_root, train_ratio, val_ratio):\n",
    "    for index, manga_title in enumerate(os.listdir(image_root)):\n",
    "        print(index, manga_title)\n",
    "        manga_path = os.path.join(image_root, manga_title)\n",
    "        \n",
    "        if not os.path.isdir(manga_path):\n",
    "            continue\n",
    "\n",
    "        # get all image files in the manga title folder\n",
    "        image_files = sorted([f for f in os.listdir(manga_path) if f.endswith('.jpg')])\n",
    "        # print(image_files)\n",
    "\n",
    "        # shuffle images to ensure random splits\n",
    "        random.shuffle(image_files)\n",
    "        # print(image_files)\n",
    "\n",
    "        # calculate the number of images for each set\n",
    "        total_images = len(image_files)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        val_count = int(total_images * val_ratio)\n",
    "        # print(total_images)\n",
    "        # print(train_count)\n",
    "        # print(val_count)\n",
    "\n",
    "        # split images into train, val, and test\n",
    "        train_files = image_files[:train_count]\n",
    "        val_files = image_files[train_count:train_count + val_count]\n",
    "        test_files = image_files[train_count + val_count:]\n",
    "        # print(train_files)\n",
    "        # print(val_files)\n",
    "        # print(test_files)\n",
    "        # print()\n",
    "\n",
    "        train_page_num = [int(str(num)[:-4]) for num in train_files]\n",
    "        val_page_num = [int(str(num)[:-4]) for num in val_files]\n",
    "        test_page_num = [int(str(num)[:-4]) for num in test_files]\n",
    "        # print(train_page_num)\n",
    "        # print(val_page_num)\n",
    "        # print(test_page_num)\n",
    "        # print()\n",
    "\n",
    "        move_files(manga_path, train_dir, train_files, manga_title, index, train_page_num)\n",
    "        print(f\"train file count: {train_count}\")\n",
    "\n",
    "        move_files(manga_path, val_dir, val_files, manga_title, index, val_page_num)\n",
    "        print(f\"val file count: {val_count}\")\n",
    "\n",
    "        move_files(manga_path, test_dir, test_files, manga_title, index, test_page_num)\n",
    "        print(f\"test file count: {val_count}\")\n",
    "\n",
    "        # print(f\"Processed {manga_title}: {train_count} train, {val_count} val, {len(test_files)} test files.\")\n",
    "        print()\n",
    "\n",
    "def move_files(src_folder, dest_folder, files, manga_title, _index, page_num):\n",
    "    for i, file in enumerate(files):\n",
    "        src_path = os.path.join(src_folder, file)\n",
    "        \n",
    "        pg_num = str(page_num[i]).zfill(5)\n",
    "        unique_id = str(_index) + pg_num[3:]\n",
    "        padded_unique_id = unique_id.zfill(5)\n",
    "        # print(padded_unique_id)\n",
    "        # print(page_num)\n",
    "        # print(str(page_num))\n",
    "\n",
    "        if page_num[i] > 99:\n",
    "            unique_id2 = str(_index) + str(page_num[i])\n",
    "            padded_unique_id2 = unique_id2.zfill(5)\n",
    "            # print(padded_unique_id2)\n",
    "            \n",
    "            new_filename = f\"{manga_title}_{padded_unique_id2}.jpg\"\n",
    "            dest_path = os.path.join(dest_folder, new_filename)\n",
    "            # print(new_filename)\n",
    "        else:\n",
    "            # print(padded_unique_id)\n",
    "            new_filename = f\"{manga_title}_{padded_unique_id}.jpg\"\n",
    "            dest_path = os.path.join(dest_folder, new_filename)\n",
    "            # print(new_filename)\n",
    "\n",
    "        # print(new_filename)\n",
    "\n",
    "        shutil.copyfile(src_path, dest_path)\n",
    "        # page_num += 1\n",
    "    # return page_num\n",
    "\n",
    "\n",
    "output_root = '../Manga109/dataset_split/'\n",
    "\n",
    "train_dir = os.path.join(output_root, 'train')\n",
    "val_dir = os.path.join(output_root, 'val')\n",
    "test_dir = os.path.join(output_root, 'test')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "image_root = '../Manga109/images/'\n",
    "train_ratio = 0.75\n",
    "val_ratio = 0.15  \n",
    "\n",
    "split_dataset(image_root, train_ratio, val_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert Manga109 annotations to COCO format for DETR usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamlet total pages at ../Manga109/dataset_split/val: 27\n",
      "Hamlet images counted: 27\n",
      "\n",
      "Finished converting xml to json format for val dataset\n",
      "Saved annotation.json file at val folder\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image  \n",
    "import re\n",
    "\n",
    "def get_photo_creation_date(file_path):\n",
    "    creation_time = os.path.getctime(file_path)\n",
    "    creation_datetime = datetime.fromtimestamp(creation_time)\n",
    "    \n",
    "    # format \"YYYY-MM-DD HH:MM:SS\"\n",
    "    formatted_date = creation_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "def convert_bbox_to_coco_format(x1, y1, x2, y2):\n",
    "    x = x1\n",
    "    y = y1\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    return [x, y, width, height]\n",
    "\n",
    "def gather_images(manga_title, split_dir, xml_root, filtered_books):\n",
    "    # print(manga_title)\n",
    "    # print('in gather_images()')\n",
    "\n",
    "    images = []\n",
    "    # page_file_list = os.listdir(split_dir)\n",
    "\n",
    "    # filtered_files = [file for file in page_file_list if file.startswith(manga_title + '_')]\n",
    "    # ['AisazuNihaIrarenai_00071.jpg', 'AisazuNihaIrarenai_00072.jpg', 'AisazuNihaIrarenai_00073.jpg', 'AisazuNihaIrarenai_00074.jpg', 'AisazuNihaIrarenai_00075.jpg', 'AisazuNihaIrarenai_00076.jpg', 'AisazuNihaIrarenai_00077.jpg', 'AisazuNihaIrarenai_00078.jpg', 'AisazuNihaIrarenai_00079.jpg', 'AisazuNihaIrarenai_00080.jpg', 'AisazuNihaIrarenai_00081.jpg', 'AisazuNihaIrarenai_00082.jpg', 'AisazuNihaIrarenai_00083.jpg', 'AisazuNihaIrarenai_00084.jpg']\n",
    "    # print(filtered_files)\n",
    "    filtered_files = [file for file in filtered_books if file.startswith(manga_title + '_')]\n",
    "    # print(filtered_files)\n",
    "    # print(len(filtered_files))\n",
    "    # print()\n",
    "\n",
    "    page_list = [page.replace('.jpg', '') for page in filtered_files]\n",
    "    # ['AisazuNihaIrarenai_00071', 'AisazuNihaIrarenai_00072', 'AisazuNihaIrarenai_00073', 'AisazuNihaIrarenai_00074', 'AisazuNihaIrarenai_00075', 'AisazuNihaIrarenai_00076', 'AisazuNihaIrarenai_00077', 'AisazuNihaIrarenai_00078', 'AisazuNihaIrarenai_00079', 'AisazuNihaIrarenai_00080', 'AisazuNihaIrarenai_00081', 'AisazuNihaIrarenai_00082', 'AisazuNihaIrarenai_00083', 'AisazuNihaIrarenai_00084']\n",
    "    # print(page_list)\n",
    "\n",
    "    page_id_str = [page.replace(str(manga_title + '_'), '') for page in page_list]\n",
    "    # ['10578', '10579', '10580', '10581', '10582', '10583', '10584', '10585', '10586', '10587', '10588', '10589', '10590', '10591']\n",
    "    # print(page_id_str)\n",
    "    # print(len(page_id_str))\n",
    "    # print()\n",
    "\n",
    "    unique_page_list_2digits = [str(page)[-2:] for page in page_id_str]\n",
    "    unique_page_list_2digits_int = [int(num) for num in unique_page_list_2digits]\n",
    "    # print('unique_page_list_2digits_int')\n",
    "    # print(unique_page_list_2digits_int)\n",
    "    # print(len(unique_page_list_2digits_int))\n",
    "    # print()\n",
    "\n",
    "    # pad the list unique_page_list_2digits_int\n",
    "    unique_page_list_2digits_int_padded = []\n",
    "    previous_num = unique_page_list_2digits_int[0]\n",
    "    for num in unique_page_list_2digits_int:\n",
    "        if num < previous_num:  # If the current number is less than the previous one, add 100\n",
    "            num += 100\n",
    "        unique_page_list_2digits_int_padded.append(num)\n",
    "        previous_num = num  # Update previous_num to current\n",
    "    \n",
    "    # go through each image annotations\n",
    "    page_split_counter = 0\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "        page_index_int = int(page_index)\n",
    "        # print(page_index_int)\n",
    "\n",
    "        if int(page_index) in unique_page_list_2digits_int_padded:\n",
    "            # print(int(page_index), unique_page_list_2digits_int_padded[page_split_counter])\n",
    "            # print(page_index, unique_page_list_2digits[0])\n",
    "            split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_id_str[page_split_counter]}.jpg\")\n",
    "            images.append({\n",
    "                \"id\": int(page_index),\n",
    "                \"width\": 1654,\n",
    "                \"height\": 1170,\n",
    "                \"file_name\": f\"{manga_title}_{page_index}.jpg\",\n",
    "                # \"date_captured\": ''\n",
    "                \"date_captured\": get_photo_creation_date(split_img_path)\n",
    "            })\n",
    "            page_split_counter += 1       \n",
    "            # print(images[-1:])\n",
    "\n",
    "    # fix numbering on the file name\n",
    "    page_id_str_counter = 0\n",
    "    for image in images:\n",
    "        detected_num_in_file_name = re.search(r'(\\d+)', image['file_name']).group(0)\n",
    "        split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_id_str[page_id_str_counter]}.jpg\")\n",
    "        # print(split_img_path)\n",
    "\n",
    "        image['id'] = int(page_id_str[page_id_str_counter])\n",
    "        image['file_name'] = image['file_name'].replace(detected_num_in_file_name, page_id_str[page_id_str_counter])\n",
    "        image['date_captured'] = get_photo_creation_date(split_img_path)\n",
    "\n",
    "        # print(image)\n",
    "        page_id_str_counter += 1\n",
    "\n",
    "    # print()\n",
    "\n",
    "    print(f'{manga_title} total pages at {split_dir}: {page_split_counter}')\n",
    "    print(f'{manga_title} images counted:', len(images))\n",
    "\n",
    "    return images\n",
    "\n",
    "def gather_annotations(manga_title, split_dir, xml_root, images):\n",
    "    # print(manga_title)\n",
    "\n",
    "    annotations = []\n",
    "    page_file_list = os.listdir(split_dir)\n",
    "\n",
    "    filtered_files = [file for file in page_file_list if file.startswith(manga_title + '_')]\n",
    "\n",
    "    page_list = [page.replace('.jpg', '') for page in filtered_files]\n",
    "    # print(page_list)\n",
    "\n",
    "    page_id_str = [page.replace(str(manga_title + '_'), '') for page in page_list]\n",
    "    # ['00346', '00347', '00348', '00349', '00350', '00351', '00352', '00353', '00354', '00357', '00358', '00359', '00360']\n",
    "    # print(page_id_str)\n",
    "\n",
    "    unique_page_list_2digits = [str(page)[-2:] for page in page_id_str]\n",
    "    # print(unique_page_list_2digits)\n",
    "    \n",
    "    objects = ['face', 'body', 'text', 'frame']\n",
    "\n",
    "    element_holder = []\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "\n",
    "        if page_index in unique_page_list_2digits:\n",
    "            # print(page_index)\n",
    "            for element in page:\n",
    "                element_holder.append(page_index)\n",
    "                bbox = element.attrib\n",
    "                category = element.tag\n",
    "                # print(bbox, bbox_tag)\n",
    "                annotations.append({\n",
    "                    \"id\": bbox['id'],\n",
    "                    \"image_id\": page_index,\n",
    "                    \"category_id\": objects.index(category),\n",
    "                    \"bbox\": convert_bbox_to_coco_format(\n",
    "                        int(bbox['xmin']), int(bbox['ymin']),\n",
    "                        int(bbox['xmax']), int(bbox['ymax'])\n",
    "                    )\n",
    "                })\n",
    "                # print(annotations[-1:])\n",
    "\n",
    "    page_id_int = list(map(int, page_id_str))\n",
    "    # print(page_id_int)\n",
    "\n",
    "    unique_page_list_2digits_int =list(map(int, unique_page_list_2digits))\n",
    "    # print(unique_page_list_2digits_int)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if int(annotation['image_id']) in unique_page_list_2digits_int:\n",
    "            annotation['image_id'] = page_id_int[unique_page_list_2digits_int.index(int(annotation['image_id']))]\n",
    "        # print(annotation)\n",
    "\n",
    "    # print(f'{manga_title} annotation count:', len(annotations))\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def gather_categories():\n",
    "    return {\n",
    "        \"categories\": [\n",
    "            {\"id\": 0, \"name\": 'face'},\n",
    "            {\"id\": 1, \"name\": 'body'},\n",
    "            {\"id\": 2, \"name\": 'text'},\n",
    "            {\"id\": 3, \"name\": 'frame'},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def save_coco_annotations(images, annotations, categories_data, coco_json_destination):\n",
    "    coco_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        **categories_data  \n",
    "    }\n",
    "    with open(coco_json_destination, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "def create_json_for_splits(fp_split_dataset):\n",
    "\n",
    "    # for split in ['train', 'val', 'test']:\n",
    "    for split in ['val']:\n",
    "        image_field = []\n",
    "        annotations_field = []\n",
    "        categories_field = gather_categories()  \n",
    "        \n",
    "        # for manga_title in os.listdir('../Manga109/images'):\n",
    "        for manga_title in ['Hamlet']:\n",
    "            # print('in create_json_for_splits()')\n",
    "            # print(manga_title)\n",
    "\n",
    "            split_dir = os.path.join(fp_split_dataset, split)\n",
    "            # print(split)\n",
    "            # print(os.listdir(split_dir))\n",
    "            # print(len(os.listdir(split_dir)))\n",
    "\n",
    "            filtered_books = [title for title in os.listdir(split_dir) if manga_title in title]\n",
    "            # print(filtered_books)\n",
    "            # print(len(filtered_books))\n",
    "            # print()\n",
    "\n",
    "            xml_path = f'../Manga109/annotations/{manga_title}.xml'\n",
    "\n",
    "            if not os.path.exists(xml_path):\n",
    "                print(f'Warning: {xml_path} does not exist. Skipping this manga title.')\n",
    "                continue\n",
    "\n",
    "            tree = ET.parse(xml_path)\n",
    "            xml_root = tree.getroot()\n",
    "\n",
    "            images = gather_images(manga_title, split_dir, xml_root, filtered_books)\n",
    "            annotations = gather_annotations(manga_title, split_dir, xml_root, images)\n",
    "            print()\n",
    "\n",
    "            image_field += images\n",
    "            annotations_field += annotations\n",
    "\n",
    "            coco_json_destination = os.path.join(split_dir, 'annotations.json')  \n",
    "        \n",
    "        print(f'Finished converting xml to json format for {split} dataset')\n",
    "        save_coco_annotations(image_field, annotations_field, categories_field, coco_json_destination)\n",
    "        print(f'Saved annotation.json file at {split} folder')\n",
    "\n",
    "fp_split_dataset = '../Manga109/dataset_split/'\n",
    "create_json_for_splits(fp_split_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
