{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split dataset to train/val/test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AisazuNihaIrarenai: 65 train, 14 val, 15 test files.\n",
      "Processed AkkeraKanjinchou: 64 train, 13 val, 15 test files.\n",
      "Processed Akuhamu: 56 train, 12 val, 13 test files.\n",
      "Processed AosugiruHaru: 73 train, 15 val, 17 test files.\n",
      "Processed AppareKappore: 67 train, 14 val, 16 test files.\n",
      "Processed Arisa: 67 train, 14 val, 16 test files.\n",
      "Processed ARMS: 56 train, 12 val, 13 test files.\n",
      "Processed BakuretsuKungFuGirl: 67 train, 14 val, 16 test files.\n",
      "Processed Belmondo: 69 train, 14 val, 16 test files.\n",
      "Processed BEMADER_P: 79 train, 17 val, 18 test files.\n",
      "Processed BokuHaSitatakaKun: 69 train, 14 val, 16 test files.\n",
      "Processed BurariTessenTorimonocho: 78 train, 16 val, 18 test files.\n",
      "Processed ByebyeC-BOY: 65 train, 14 val, 15 test files.\n",
      "Processed Count3DeKimeteAgeru: 69 train, 14 val, 16 test files.\n",
      "Processed DollGun: 67 train, 14 val, 15 test files.\n",
      "Processed Donburakokko: 62 train, 13 val, 14 test files.\n",
      "Processed DualJustice: 69 train, 14 val, 16 test files.\n",
      "Processed EienNoWith: 87 train, 18 val, 20 test files.\n",
      "Processed EvaLady: 76 train, 16 val, 17 test files.\n",
      "Processed EverydayOsakanaChan: 60 train, 12 val, 14 test files.\n",
      "Processed GakuenNoise: 69 train, 14 val, 16 test files.\n",
      "Processed GarakutayaManta: 72 train, 15 val, 16 test files.\n",
      "Processed GinNoChimera: 72 train, 15 val, 16 test files.\n",
      "Processed GOOD_KISS_Ver2: 66 train, 14 val, 15 test files.\n",
      "Processed Hamlet: 129 train, 27 val, 29 test files.\n",
      "Processed HanzaiKousyouninMinegishiEitarou: 70 train, 15 val, 15 test files.\n",
      "Processed HaruichibanNoFukukoro: 67 train, 14 val, 16 test files.\n",
      "Processed HarukaRefrain: 69 train, 14 val, 16 test files.\n",
      "Processed HealingPlanet: 66 train, 14 val, 15 test files.\n",
      "Processed HeiseiJimen: 77 train, 16 val, 18 test files.\n",
      "Processed HighschoolKimengumi_vol01: 69 train, 14 val, 16 test files.\n",
      "Processed HighschoolKimengumi_vol20: 65 train, 14 val, 15 test files.\n",
      "Processed HinagikuKenzan: 62 train, 13 val, 14 test files.\n",
      "Processed HisokaReturns: 62 train, 13 val, 15 test files.\n",
      "Processed JangiriPonpon: 77 train, 16 val, 17 test files.\n",
      "Processed JijiBabaFight: 51 train, 10 val, 12 test files.\n",
      "Processed Joouari: 67 train, 14 val, 16 test files.\n",
      "Processed Jyovolley: 65 train, 13 val, 15 test files.\n",
      "Processed KarappoHighschool: 84 train, 18 val, 19 test files.\n",
      "Processed KimiHaBokuNoTaiyouDa: 77 train, 16 val, 18 test files.\n",
      "Processed KoukouNoHitotachi: 46 train, 10 val, 11 test files.\n",
      "Processed KuroidoGanka: 70 train, 15 val, 16 test files.\n",
      "Processed KyokugenCyclone: 40 train, 8 val, 10 test files.\n",
      "Processed LancelotFullThrottle: 67 train, 14 val, 16 test files.\n",
      "Processed LoveHina_vol01: 67 train, 14 val, 16 test files.\n",
      "Processed LoveHina_vol14: 67 train, 14 val, 16 test files.\n",
      "Processed MadouTaiga: 58 train, 12 val, 14 test files.\n",
      "Processed MAD_STONE: 67 train, 14 val, 15 test files.\n",
      "Processed MagicianLoad: 61 train, 13 val, 14 test files.\n",
      "Processed MagicStarGakuin: 57 train, 12 val, 13 test files.\n",
      "Processed MariaSamaNihaNaisyo: 67 train, 14 val, 16 test files.\n",
      "Processed MayaNoAkaiKutsu: 62 train, 13 val, 15 test files.\n",
      "Processed MemorySeijin: 71 train, 15 val, 16 test files.\n",
      "Processed MeteoSanStrikeDesu: 79 train, 17 val, 18 test files.\n",
      "Processed MiraiSan: 74 train, 16 val, 17 test files.\n",
      "Processed MisutenaideDaisy: 77 train, 16 val, 18 test files.\n",
      "Processed MoeruOnisan_vol01: 64 train, 13 val, 15 test files.\n",
      "Processed MoeruOnisan_vol19: 69 train, 14 val, 16 test files.\n",
      "Processed MomoyamaHaikagura: 35 train, 7 val, 9 test files.\n",
      "Processed MukoukizuNoChonbo: 44 train, 9 val, 10 test files.\n",
      "Processed MutekiBoukenSyakuma: 65 train, 13 val, 15 test files.\n",
      "Processed Nekodama: 81 train, 17 val, 19 test files.\n",
      "Processed NichijouSoup: 71 train, 15 val, 16 test files.\n",
      "Processed Ningyoushi: 66 train, 14 val, 15 test files.\n",
      "Processed OhWareraRettouSeitokai: 67 train, 14 val, 15 test files.\n",
      "Processed OL_Lunch: 46 train, 10 val, 11 test files.\n",
      "Processed ParaisoRoad: 73 train, 15 val, 17 test files.\n",
      "Processed PikaruGenkiDesu: 65 train, 13 val, 15 test files.\n",
      "Processed PLANET7: 69 train, 14 val, 16 test files.\n",
      "Processed PlatinumJungle: 60 train, 12 val, 14 test files.\n",
      "Processed PrayerHaNemurenai: 68 train, 14 val, 16 test files.\n",
      "Processed PrismHeart: 64 train, 13 val, 15 test files.\n",
      "Processed PsychoStaff: 63 train, 13 val, 15 test files.\n",
      "Processed Raphael: 81 train, 17 val, 19 test files.\n",
      "Processed ReveryEarth: 70 train, 15 val, 15 test files.\n",
      "Processed RinToSiteSippuNoNaka: 106 train, 22 val, 24 test files.\n",
      "Processed RisingGirl: 84 train, 18 val, 19 test files.\n",
      "Processed Saisoku: 70 train, 15 val, 15 test files.\n",
      "Processed SaladDays_vol01: 63 train, 13 val, 15 test files.\n",
      "Processed SaladDays_vol18: 66 train, 14 val, 15 test files.\n",
      "Processed SamayoeruSyonenNiJunaiWo: 62 train, 13 val, 14 test files.\n",
      "Processed SeisinkiVulnus: 70 train, 15 val, 15 test files.\n",
      "Processed ShimatteIkouze_vol01: 75 train, 16 val, 17 test files.\n",
      "Processed ShimatteIkouze_vol26: 78 train, 16 val, 18 test files.\n",
      "Processed SonokiDeABC: 69 train, 14 val, 16 test files.\n",
      "Processed SyabondamaKieta: 65 train, 14 val, 15 test files.\n",
      "Processed TaiyouNiSmash: 63 train, 13 val, 15 test files.\n",
      "Processed TapkunNoTanteisitsu: 74 train, 16 val, 17 test files.\n",
      "Processed TasogareTsushin: 68 train, 14 val, 16 test files.\n",
      "Processed TennenSenshiG: 62 train, 13 val, 14 test files.\n",
      "Processed TensiNoHaneToAkumaNoShippo: 63 train, 13 val, 15 test files.\n",
      "Processed TetsuSan: 28 train, 6 val, 6 test files.\n",
      "Processed That'sIzumiko: 88 train, 19 val, 20 test files.\n",
      "Processed TotteokiNoABC: 65 train, 14 val, 15 test files.\n",
      "Processed ToutaMairimasu: 76 train, 16 val, 17 test files.\n",
      "Processed TouyouKidan: 93 train, 19 val, 21 test files.\n",
      "Processed TsubasaNoKioku: 67 train, 14 val, 16 test files.\n",
      "Processed UchiNoNyan'sDiary: 57 train, 12 val, 13 test files.\n",
      "Processed UchuKigekiM774: 58 train, 12 val, 13 test files.\n",
      "Processed UltraEleven: 79 train, 16 val, 18 test files.\n",
      "Processed UnbalanceTokyo: 61 train, 13 val, 14 test files.\n",
      "Processed WarewareHaOniDearu: 67 train, 14 val, 15 test files.\n",
      "Processed YamatoNoHane: 79 train, 17 val, 18 test files.\n",
      "Processed YasasiiAkuma: 65 train, 14 val, 15 test files.\n",
      "Processed YouchienBoueigumi: 18 train, 4 val, 5 test files.\n",
      "Processed YoumaKourin: 74 train, 16 val, 17 test files.\n",
      "Processed YukiNoFuruMachi: 66 train, 14 val, 15 test files.\n",
      "Processed YumeiroCooking: 67 train, 14 val, 15 test files.\n",
      "Processed YumeNoKayoiji: 67 train, 14 val, 16 test files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define paths and ratios\n",
    "image_root = '../Manga109/images/'\n",
    "output_root = '../Manga109/dataset_split/'\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15  \n",
    "\n",
    "train_dir = os.path.join(output_root, 'train')\n",
    "val_dir = os.path.join(output_root, 'val')\n",
    "test_dir = os.path.join(output_root, 'test')\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "def split_dataset(image_root, train_ratio, val_ratio):\n",
    "    for manga_title in os.listdir(image_root):\n",
    "        manga_path = os.path.join(image_root, manga_title)\n",
    "        \n",
    "        # Skip non-directory items\n",
    "        if not os.path.isdir(manga_path):\n",
    "            continue\n",
    "\n",
    "        # Get all image files in the manga title folder\n",
    "        image_files = sorted([f for f in os.listdir(manga_path) if f.endswith('.jpg')])\n",
    "\n",
    "        # Shuffle images to ensure random splits\n",
    "        random.shuffle(image_files)\n",
    "\n",
    "        # Calculate the number of images for each set\n",
    "        total_images = len(image_files)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        val_count = int(total_images * val_ratio)\n",
    "\n",
    "        # Split images into train, val, and test\n",
    "        train_files = image_files[:train_count]\n",
    "        val_files = image_files[train_count:train_count + val_count]\n",
    "        test_files = image_files[train_count + val_count:]\n",
    "\n",
    "        # Move and rename files to the corresponding split folders\n",
    "        move_files(manga_path, train_dir, train_files, manga_title)\n",
    "        move_files(manga_path, val_dir, val_files, manga_title)\n",
    "        move_files(manga_path, test_dir, test_files, manga_title)\n",
    "\n",
    "        print(f\"Processed {manga_title}: {train_count} train, {val_count} val, {len(test_files)} test files.\")\n",
    "\n",
    "def move_files(src_folder, dest_folder, files, manga_title):\n",
    "    for idx, file in enumerate(files):\n",
    "        src_path = os.path.join(src_folder, file)\n",
    "        \n",
    "        # Create new filename using manga title and index\n",
    "        new_filename = f\"{manga_title}_{idx:03}.jpg\"\n",
    "        dest_path = os.path.join(dest_folder, new_filename)\n",
    "\n",
    "        # Copy file to the destination folder with new name\n",
    "        shutil.copyfile(src_path, dest_path)\n",
    "\n",
    "split_dataset(image_root, train_ratio, val_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert Manga109 annotations to COCO format for DETR usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished converting xml to json format for train dataset\n",
      "Saved annotation.json file at train folder\n",
      "Finished converting xml to json format for val dataset\n",
      "Saved annotation.json file at val folder\n",
      "Finished converting xml to json format for test dataset\n",
      "Saved annotation.json file at test folder\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image  \n",
    "\n",
    "def get_photo_creation_date(file_path):\n",
    "    creation_time = os.path.getctime(file_path)\n",
    "    creation_datetime = datetime.fromtimestamp(creation_time)\n",
    "    \n",
    "    # format \"YYYY-MM-DD HH:MM:SS\"\n",
    "    formatted_date = creation_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "def convert_bbox_to_coco_format(x1, y1, x2, y2):\n",
    "    x = x1\n",
    "    y = y1\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    return [x, y, width, height]\n",
    "\n",
    "def gather_images(manga_title, split_dir, xml_root):\n",
    "    images = []\n",
    "    page_file_list = os.listdir(split_dir)\n",
    "    page_list = [page.replace('.jpg', '') for page in page_file_list]\n",
    "\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "        page_index_padded = page_index.zfill(3)\n",
    "\n",
    "        if f\"{manga_title}_{page_index_padded}\" in page_list:\n",
    "            split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_index_padded}.jpg\")\n",
    "            images.append({\n",
    "                \"id\": int(page_index),\n",
    "                \"width\": 1654,\n",
    "                \"height\": 1170,\n",
    "                \"file_name\": f\"{manga_title}_{page_index_padded}.jpg\",\n",
    "                \"date_captured\": get_photo_creation_date(split_img_path)\n",
    "            })\n",
    "\n",
    "    return images\n",
    "\n",
    "def gather_annotations(manga_title, split_dir, xml_root):\n",
    "    annotations = []\n",
    "    page_file_list = os.listdir(split_dir)\n",
    "\n",
    "    page_list = [page.replace('.jpg', '') for page in page_file_list]\n",
    "    objects = ['face', 'body', 'text', 'frame']\n",
    "\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "        page_index_padded = page_index.zfill(3)\n",
    "\n",
    "        if f\"{manga_title}_{page_index_padded}\" in page_list:\n",
    "            for textbox in page:\n",
    "                bbox = textbox.attrib\n",
    "                annotations.append({\n",
    "                    \"id\": bbox['id'],\n",
    "                    \"image_id\": int(page_index),\n",
    "                    \"category_id\": objects.index(textbox.tag),\n",
    "                    \"bbox\": convert_bbox_to_coco_format(\n",
    "                        int(bbox['xmin']), int(bbox['ymin']),\n",
    "                        int(bbox['xmax']), int(bbox['ymax'])\n",
    "                    )\n",
    "                })\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def gather_categories():\n",
    "    return {\n",
    "        \"categories\": [\n",
    "            {\"id\": 0, \"name\": 'face'},\n",
    "            {\"id\": 1, \"name\": 'body'},\n",
    "            {\"id\": 2, \"name\": 'text'},\n",
    "            {\"id\": 3, \"name\": 'frame'},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def save_coco_annotations(images, annotations, categories_data, coco_json_destination):\n",
    "    coco_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        **categories_data  \n",
    "    }\n",
    "\n",
    "    # Write to JSON file\n",
    "    with open(coco_json_destination, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "def create_json_for_splits(fp_split_dataset):\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        image_field = []\n",
    "        annotations_field = []\n",
    "        categories_field = gather_categories()  \n",
    "        \n",
    "        for manga_title in os.listdir('../Manga109/images'):\n",
    "            split_dir = os.path.join(fp_split_dataset, split)\n",
    "            xml_path = f'../Manga109/annotations/{manga_title}.xml'\n",
    "\n",
    "            if not os.path.exists(xml_path):\n",
    "                print(f'Warning: {xml_path} does not exist. Skipping this manga title.')\n",
    "                continue\n",
    "\n",
    "            tree = ET.parse(xml_path)\n",
    "            xml_root = tree.getroot()\n",
    "\n",
    "            images = gather_images(manga_title, split_dir, xml_root)\n",
    "            annotations = gather_annotations(manga_title, split_dir, xml_root)\n",
    "\n",
    "            image_field += images\n",
    "            annotations_field += annotations\n",
    "\n",
    "            coco_json_destination = os.path.join(split_dir, 'annotations.json')  \n",
    "        \n",
    "        print(f'Finished converting xml to json format for {split} dataset')\n",
    "        save_coco_annotations(image_field, annotations_field, categories_field, coco_json_destination)\n",
    "        print(f'Saved annotation.json file at {split} folder')\n",
    "\n",
    "fp_split_dataset = '../Manga109/dataset_split/'\n",
    "create_json_for_splits(fp_split_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
