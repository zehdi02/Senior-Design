{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split dataset to train/val/test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed AisazuNihaIrarenai: 70 train, 14 val, 10 test files.\n",
      "Processed AkkeraKanjinchou: 69 train, 13 val, 10 test files.\n",
      "Processed Akuhamu: 60 train, 12 val, 9 test files.\n",
      "Processed AosugiruHaru: 78 train, 15 val, 12 test files.\n",
      "Processed AppareKappore: 72 train, 14 val, 11 test files.\n",
      "Processed Arisa: 72 train, 14 val, 11 test files.\n",
      "Processed ARMS: 60 train, 12 val, 9 test files.\n",
      "Processed BakuretsuKungFuGirl: 72 train, 14 val, 11 test files.\n",
      "Processed Belmondo: 74 train, 14 val, 11 test files.\n",
      "Processed BEMADER_P: 85 train, 17 val, 12 test files.\n",
      "Processed BokuHaSitatakaKun: 74 train, 14 val, 11 test files.\n",
      "Processed BurariTessenTorimonocho: 84 train, 16 val, 12 test files.\n",
      "Processed ByebyeC-BOY: 70 train, 14 val, 10 test files.\n",
      "Processed Count3DeKimeteAgeru: 74 train, 14 val, 11 test files.\n",
      "Processed DollGun: 72 train, 14 val, 10 test files.\n",
      "Processed Donburakokko: 66 train, 13 val, 10 test files.\n",
      "Processed DualJustice: 74 train, 14 val, 11 test files.\n",
      "Processed EienNoWith: 93 train, 18 val, 14 test files.\n",
      "Processed EvaLady: 81 train, 16 val, 12 test files.\n",
      "Processed EverydayOsakanaChan: 64 train, 12 val, 10 test files.\n",
      "Processed GakuenNoise: 74 train, 14 val, 11 test files.\n",
      "Processed GarakutayaManta: 77 train, 15 val, 11 test files.\n",
      "Processed GinNoChimera: 77 train, 15 val, 11 test files.\n",
      "Processed GOOD_KISS_Ver2: 71 train, 14 val, 10 test files.\n",
      "Processed Hamlet: 138 train, 27 val, 20 test files.\n",
      "Processed HanzaiKousyouninMinegishiEitarou: 75 train, 15 val, 10 test files.\n",
      "Processed HaruichibanNoFukukoro: 72 train, 14 val, 11 test files.\n",
      "Processed HarukaRefrain: 74 train, 14 val, 11 test files.\n",
      "Processed HealingPlanet: 71 train, 14 val, 10 test files.\n",
      "Processed HeiseiJimen: 83 train, 16 val, 12 test files.\n",
      "Processed HighschoolKimengumi_vol01: 74 train, 14 val, 11 test files.\n",
      "Processed HighschoolKimengumi_vol20: 70 train, 14 val, 10 test files.\n",
      "Processed HinagikuKenzan: 66 train, 13 val, 10 test files.\n",
      "Processed HisokaReturns: 67 train, 13 val, 10 test files.\n",
      "Processed JangiriPonpon: 82 train, 16 val, 12 test files.\n",
      "Processed JijiBabaFight: 54 train, 10 val, 9 test files.\n",
      "Processed Joouari: 72 train, 14 val, 11 test files.\n",
      "Processed Jyovolley: 69 train, 13 val, 11 test files.\n",
      "Processed KarappoHighschool: 90 train, 18 val, 13 test files.\n",
      "Processed KimiHaBokuNoTaiyouDa: 83 train, 16 val, 12 test files.\n",
      "Processed KoukouNoHitotachi: 50 train, 10 val, 7 test files.\n",
      "Processed KuroidoGanka: 75 train, 15 val, 11 test files.\n",
      "Processed KyokugenCyclone: 43 train, 8 val, 7 test files.\n",
      "Processed LancelotFullThrottle: 72 train, 14 val, 11 test files.\n",
      "Processed LoveHina_vol01: 72 train, 14 val, 11 test files.\n",
      "Processed LoveHina_vol14: 72 train, 14 val, 11 test files.\n",
      "Processed MadouTaiga: 63 train, 12 val, 9 test files.\n",
      "Processed MAD_STONE: 72 train, 14 val, 10 test files.\n",
      "Processed MagicianLoad: 66 train, 13 val, 9 test files.\n",
      "Processed MagicStarGakuin: 61 train, 12 val, 9 test files.\n",
      "Processed MariaSamaNihaNaisyo: 72 train, 14 val, 11 test files.\n",
      "Processed MayaNoAkaiKutsu: 67 train, 13 val, 10 test files.\n",
      "Processed MemorySeijin: 76 train, 15 val, 11 test files.\n",
      "Processed MeteoSanStrikeDesu: 85 train, 17 val, 12 test files.\n",
      "Processed MiraiSan: 80 train, 16 val, 11 test files.\n",
      "Processed MisutenaideDaisy: 83 train, 16 val, 12 test files.\n",
      "Processed MoeruOnisan_vol01: 69 train, 13 val, 10 test files.\n",
      "Processed MoeruOnisan_vol19: 74 train, 14 val, 11 test files.\n",
      "Processed MomoyamaHaikagura: 38 train, 7 val, 6 test files.\n",
      "Processed MukoukizuNoChonbo: 47 train, 9 val, 7 test files.\n",
      "Processed MutekiBoukenSyakuma: 69 train, 13 val, 11 test files.\n",
      "Processed Nekodama: 87 train, 17 val, 13 test files.\n",
      "Processed NichijouSoup: 76 train, 15 val, 11 test files.\n",
      "Processed Ningyoushi: 71 train, 14 val, 10 test files.\n",
      "Processed OhWareraRettouSeitokai: 72 train, 14 val, 10 test files.\n",
      "Processed OL_Lunch: 50 train, 10 val, 7 test files.\n",
      "Processed ParaisoRoad: 78 train, 15 val, 12 test files.\n",
      "Processed PikaruGenkiDesu: 69 train, 13 val, 11 test files.\n",
      "Processed PLANET7: 74 train, 14 val, 11 test files.\n",
      "Processed PlatinumJungle: 64 train, 12 val, 10 test files.\n",
      "Processed PrayerHaNemurenai: 73 train, 14 val, 11 test files.\n",
      "Processed PrismHeart: 69 train, 13 val, 10 test files.\n",
      "Processed PsychoStaff: 68 train, 13 val, 10 test files.\n",
      "Processed Raphael: 87 train, 17 val, 13 test files.\n",
      "Processed ReveryEarth: 75 train, 15 val, 10 test files.\n",
      "Processed RinToSiteSippuNoNaka: 114 train, 22 val, 16 test files.\n",
      "Processed RisingGirl: 90 train, 18 val, 13 test files.\n",
      "Processed Saisoku: 75 train, 15 val, 10 test files.\n",
      "Processed SaladDays_vol01: 68 train, 13 val, 10 test files.\n",
      "Processed SaladDays_vol18: 71 train, 14 val, 10 test files.\n",
      "Processed SamayoeruSyonenNiJunaiWo: 66 train, 13 val, 10 test files.\n",
      "Processed SeisinkiVulnus: 75 train, 15 val, 10 test files.\n",
      "Processed ShimatteIkouze_vol01: 81 train, 16 val, 11 test files.\n",
      "Processed ShimatteIkouze_vol26: 84 train, 16 val, 12 test files.\n",
      "Processed SonokiDeABC: 74 train, 14 val, 11 test files.\n",
      "Processed SyabondamaKieta: 70 train, 14 val, 10 test files.\n",
      "Processed TaiyouNiSmash: 68 train, 13 val, 10 test files.\n",
      "Processed TapkunNoTanteisitsu: 80 train, 16 val, 11 test files.\n",
      "Processed TasogareTsushin: 73 train, 14 val, 11 test files.\n",
      "Processed TennenSenshiG: 66 train, 13 val, 10 test files.\n",
      "Processed TensiNoHaneToAkumaNoShippo: 68 train, 13 val, 10 test files.\n",
      "Processed TetsuSan: 30 train, 6 val, 4 test files.\n",
      "Processed That'sIzumiko: 95 train, 19 val, 13 test files.\n",
      "Processed TotteokiNoABC: 70 train, 14 val, 10 test files.\n",
      "Processed ToutaMairimasu: 81 train, 16 val, 12 test files.\n",
      "Processed TouyouKidan: 99 train, 19 val, 15 test files.\n",
      "Processed TsubasaNoKioku: 72 train, 14 val, 11 test files.\n",
      "Processed UchiNoNyan'sDiary: 61 train, 12 val, 9 test files.\n",
      "Processed UchuKigekiM774: 62 train, 12 val, 9 test files.\n",
      "Processed UltraEleven: 84 train, 16 val, 13 test files.\n",
      "Processed UnbalanceTokyo: 66 train, 13 val, 9 test files.\n",
      "Processed WarewareHaOniDearu: 72 train, 14 val, 10 test files.\n",
      "Processed YamatoNoHane: 85 train, 17 val, 12 test files.\n",
      "Processed YasasiiAkuma: 70 train, 14 val, 10 test files.\n",
      "Processed YouchienBoueigumi: 20 train, 4 val, 3 test files.\n",
      "Processed YoumaKourin: 80 train, 16 val, 11 test files.\n",
      "Processed YukiNoFuruMachi: 71 train, 14 val, 10 test files.\n",
      "Processed YumeiroCooking: 72 train, 14 val, 10 test files.\n",
      "Processed YumeNoKayoiji: 72 train, 14 val, 11 test files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(image_root, train_ratio, val_ratio):\n",
    "    global unique_id_counter  \n",
    "    unique_id_counter = 1  # Start counting IDs from 1\n",
    "\n",
    "    for manga_title in os.listdir(image_root):\n",
    "        manga_path = os.path.join(image_root, manga_title)\n",
    "        \n",
    "        if not os.path.isdir(manga_path):\n",
    "            continue\n",
    "\n",
    "        # Get all image files in the manga title folder\n",
    "        image_files = sorted([f for f in os.listdir(manga_path) if f.endswith('.jpg')])\n",
    "\n",
    "        # Shuffle images to ensure random splits\n",
    "        random.shuffle(image_files)\n",
    "\n",
    "        # Calculate the number of images for each set\n",
    "        total_images = len(image_files)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        val_count = int(total_images * val_ratio)\n",
    "\n",
    "        # Split images into train, val, and test\n",
    "        train_files = image_files[:train_count]\n",
    "        val_files = image_files[train_count:train_count + val_count]\n",
    "        test_files = image_files[train_count + val_count:]\n",
    "\n",
    "        # Move and rename files to the corresponding split folders\n",
    "        move_files(manga_path, train_dir, train_files, manga_title)\n",
    "        move_files(manga_path, val_dir, val_files, manga_title)\n",
    "        move_files(manga_path, test_dir, test_files, manga_title)\n",
    "\n",
    "        print(f\"Processed {manga_title}: {train_count} train, {val_count} val, {len(test_files)} test files.\")\n",
    "\n",
    "def move_files(src_folder, dest_folder, files, manga_title):\n",
    "    global unique_id_counter  \n",
    "    for file in files:\n",
    "        src_path = os.path.join(src_folder, file)\n",
    "        \n",
    "        # Use the global unique ID counter as part of the filename with 5 digits\n",
    "        new_filename = f\"{manga_title}_{unique_id_counter:05}.jpg\"\n",
    "        dest_path = os.path.join(dest_folder, new_filename)\n",
    "\n",
    "        # Copy file to the destination folder with the new name\n",
    "        shutil.copyfile(src_path, dest_path)\n",
    "\n",
    "        unique_id_counter += 1  # Increment the counter for the next unique ID\n",
    "\n",
    "# Define output directories\n",
    "output_root = '../Manga109/dataset_split/'\n",
    "\n",
    "train_dir = os.path.join(output_root, 'train')\n",
    "val_dir = os.path.join(output_root, 'val')\n",
    "test_dir = os.path.join(output_root, 'test')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Define image root and dataset split ratios\n",
    "image_root = '../Manga109/images/'\n",
    "train_ratio = 0.75\n",
    "val_ratio = 0.15  \n",
    "\n",
    "# Split the dataset and rename files with standardized IDs\n",
    "split_dataset(image_root, train_ratio, val_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert Manga109 annotations to COCO format for DETR usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image  \n",
    "import re\n",
    "\n",
    "def get_photo_creation_date(file_path):\n",
    "    creation_time = os.path.getctime(file_path)\n",
    "    creation_datetime = datetime.fromtimestamp(creation_time)\n",
    "    \n",
    "    # format \"YYYY-MM-DD HH:MM:SS\"\n",
    "    formatted_date = creation_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "def convert_bbox_to_coco_format(x1, y1, x2, y2):\n",
    "    x = x1\n",
    "    y = y1\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    return [x, y, width, height]\n",
    "\n",
    "def gather_images(manga_title, split_dir, xml_root):\n",
    "    # print(manga_title)\n",
    "\n",
    "    images = []\n",
    "    page_file_list = os.listdir(split_dir)\n",
    "\n",
    "    filtered_files = [file for file in page_file_list if file.startswith(manga_title + '_')]\n",
    "    # ['AisazuNihaIrarenai_00071.jpg', 'AisazuNihaIrarenai_00072.jpg', 'AisazuNihaIrarenai_00073.jpg', 'AisazuNihaIrarenai_00074.jpg', 'AisazuNihaIrarenai_00075.jpg', 'AisazuNihaIrarenai_00076.jpg', 'AisazuNihaIrarenai_00077.jpg', 'AisazuNihaIrarenai_00078.jpg', 'AisazuNihaIrarenai_00079.jpg', 'AisazuNihaIrarenai_00080.jpg', 'AisazuNihaIrarenai_00081.jpg', 'AisazuNihaIrarenai_00082.jpg', 'AisazuNihaIrarenai_00083.jpg', 'AisazuNihaIrarenai_00084.jpg']\n",
    "    # print(filtered_files)\n",
    "\n",
    "    page_list = [page.replace('.jpg', '') for page in filtered_files]\n",
    "    # ['AisazuNihaIrarenai_00071', 'AisazuNihaIrarenai_00072', 'AisazuNihaIrarenai_00073', 'AisazuNihaIrarenai_00074', 'AisazuNihaIrarenai_00075', 'AisazuNihaIrarenai_00076', 'AisazuNihaIrarenai_00077', 'AisazuNihaIrarenai_00078', 'AisazuNihaIrarenai_00079', 'AisazuNihaIrarenai_00080', 'AisazuNihaIrarenai_00081', 'AisazuNihaIrarenai_00082', 'AisazuNihaIrarenai_00083', 'AisazuNihaIrarenai_00084']\n",
    "    # print(page_list)\n",
    "\n",
    "    page_id_str = [page.replace(str(manga_title + '_'), '') for page in page_list]\n",
    "    # ['10578', '10579', '10580', '10581', '10582', '10583', '10584', '10585', '10586', '10587', '10588', '10589', '10590', '10591']\n",
    "    # print(page_id_str)\n",
    "\n",
    "    unique_page_list_2digits = [str(page)[-2:] for page in page_id_str]\n",
    "\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "        # page_index_padded = page_index.zfill(5)\n",
    "\n",
    "        if page_index in unique_page_list_2digits:\n",
    "            # print(page_index)\n",
    "\n",
    "        # if f\"{manga_title}_{page_index_padded}\" in page_id_str:\n",
    "            # split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_index_padded}.jpg\")\n",
    "            images.append({\n",
    "                \"id\": int(page_index),\n",
    "                \"width\": 1654,\n",
    "                \"height\": 1170,\n",
    "                \"file_name\": f\"{manga_title}_{page_index}.jpg\",\n",
    "                \"date_captured\": ''\n",
    "                # \"date_captured\": get_photo_creation_date(split_img_path)\n",
    "            })\n",
    "            # print(images[-1:])\n",
    "\n",
    "    # fix numbering on the file name\n",
    "    page_id_str_counter = 0\n",
    "    for image in images:\n",
    "        detected_num_in_file_name = re.search(r'(\\d+)', image['file_name']).group(0)\n",
    "        split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_id_str[page_id_str_counter]}.jpg\")\n",
    "        # print(split_img_path)\n",
    "\n",
    "        image['id'] = int(page_id_str[page_id_str_counter])\n",
    "        image['file_name'] = image['file_name'].replace(detected_num_in_file_name, page_id_str[page_id_str_counter])\n",
    "        image['date_captured'] = get_photo_creation_date(split_img_path)\n",
    "\n",
    "        # print(image)\n",
    "        page_id_str_counter += 1\n",
    "\n",
    "    print()\n",
    "\n",
    "    return images\n",
    "\n",
    "def gather_annotations(manga_title, split_dir, xml_root, images):\n",
    "    # print(manga_title)\n",
    "\n",
    "    annotations = []\n",
    "    page_file_list = os.listdir(split_dir)\n",
    "\n",
    "    filtered_files = [file for file in page_file_list if file.startswith(manga_title + '_')]\n",
    "\n",
    "    page_list = [page.replace('.jpg', '') for page in filtered_files]\n",
    "    # print(page_list)\n",
    "\n",
    "    page_id_str = [page.replace(str(manga_title + '_'), '') for page in page_list]\n",
    "    # ['00346', '00347', '00348', '00349', '00350', '00351', '00352', '00353', '00354', '00357', '00358', '00359', '00360']\n",
    "    # print(page_id_str)\n",
    "\n",
    "    unique_page_list_2digits = [str(page)[-2:] for page in page_id_str]\n",
    "    # print(unique_page_list_2digits)\n",
    "    \n",
    "    objects = ['face', 'body', 'text', 'frame']\n",
    "\n",
    "    element_holder = []\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "\n",
    "        if page_index in unique_page_list_2digits:\n",
    "            # print(page_index)\n",
    "            for element in page:\n",
    "                element_holder.append(page_index)\n",
    "                bbox = element.attrib\n",
    "                category = element.tag\n",
    "                # print(bbox, bbox_tag)\n",
    "                annotations.append({\n",
    "                    \"id\": bbox['id'],\n",
    "                    \"image_id\": page_index,\n",
    "                    \"category_id\": objects.index(category),\n",
    "                    \"bbox\": convert_bbox_to_coco_format(\n",
    "                        int(bbox['xmin']), int(bbox['ymin']),\n",
    "                        int(bbox['xmax']), int(bbox['ymax'])\n",
    "                    )\n",
    "                })\n",
    "                # print(annotations[-1:])\n",
    "\n",
    "    page_id_int = list(map(int, page_id_str))\n",
    "    # print(page_id_int)\n",
    "\n",
    "    unique_page_list_2digits_int =list(map(int, unique_page_list_2digits))\n",
    "    # print(unique_page_list_2digits_int)\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if int(annotation['image_id']) in unique_page_list_2digits_int:\n",
    "            annotation['image_id'] = page_id_int[unique_page_list_2digits_int.index(int(annotation['image_id']))]\n",
    "        # print(annotation)\n",
    "\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def gather_categories():\n",
    "    return {\n",
    "        \"categories\": [\n",
    "            {\"id\": 0, \"name\": 'face'},\n",
    "            {\"id\": 1, \"name\": 'body'},\n",
    "            {\"id\": 2, \"name\": 'text'},\n",
    "            {\"id\": 3, \"name\": 'frame'},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def save_coco_annotations(images, annotations, categories_data, coco_json_destination):\n",
    "    coco_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        **categories_data  \n",
    "    }\n",
    "\n",
    "    # Write to JSON file\n",
    "    with open(coco_json_destination, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "def create_json_for_splits(fp_split_dataset):\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "    # for split in ['val']:\n",
    "        image_field = []\n",
    "        annotations_field = []\n",
    "        categories_field = gather_categories()  \n",
    "        \n",
    "        for manga_title in os.listdir('../Manga109/images'):\n",
    "            # print(manga_title)\n",
    "\n",
    "            split_dir = os.path.join(fp_split_dataset, split)\n",
    "            xml_path = f'../Manga109/annotations/{manga_title}.xml'\n",
    "\n",
    "            if not os.path.exists(xml_path):\n",
    "                print(f'Warning: {xml_path} does not exist. Skipping this manga title.')\n",
    "                continue\n",
    "\n",
    "            tree = ET.parse(xml_path)\n",
    "            xml_root = tree.getroot()\n",
    "\n",
    "            images = gather_images(manga_title, split_dir, xml_root)\n",
    "            annotations = gather_annotations(manga_title, split_dir, xml_root, images)\n",
    "\n",
    "            image_field += images\n",
    "            annotations_field += annotations\n",
    "\n",
    "            coco_json_destination = os.path.join(split_dir, 'annotations.json')  \n",
    "        \n",
    "        print(f'Finished converting xml to json format for {split} dataset')\n",
    "        save_coco_annotations(image_field, annotations_field, categories_field, coco_json_destination)\n",
    "        print(f'Saved annotation.json file at {split} folder')\n",
    "\n",
    "fp_split_dataset = '../Manga109/dataset_split/'\n",
    "create_json_for_splits(fp_split_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
