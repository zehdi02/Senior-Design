{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\zed\\anaconda3\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\zed\\anaconda3\\lib\\site-packages (0.19.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\zed\\anaconda3\\lib\\site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\zed\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\zed\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zed\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split dataset to train/val/test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AisazuNihaIrarenai\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "1 AkkeraKanjinchou\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "2 Akuhamu\n",
      "train file count: 60\n",
      "val file count: 12\n",
      "test file count: 9\n",
      "\n",
      "3 AosugiruHaru\n",
      "train file count: 78\n",
      "val file count: 15\n",
      "test file count: 12\n",
      "\n",
      "4 AppareKappore\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "5 Arisa\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "6 ARMS\n",
      "train file count: 60\n",
      "val file count: 12\n",
      "test file count: 9\n",
      "\n",
      "7 BakuretsuKungFuGirl\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "8 Belmondo\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "9 BEMADER_P\n",
      "train file count: 85\n",
      "val file count: 17\n",
      "test file count: 12\n",
      "\n",
      "10 BokuHaSitatakaKun\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "11 BurariTessenTorimonocho\n",
      "train file count: 84\n",
      "val file count: 16\n",
      "test file count: 12\n",
      "\n",
      "12 ByebyeC-BOY\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "13 DollGun\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "14 Donburakokko\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "15 DualJustice\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "16 EienNoWith\n",
      "train file count: 93\n",
      "val file count: 18\n",
      "test file count: 14\n",
      "\n",
      "17 EvaLady\n",
      "train file count: 81\n",
      "val file count: 16\n",
      "test file count: 12\n",
      "\n",
      "18 EverydayOsakanaChan\n",
      "train file count: 64\n",
      "val file count: 12\n",
      "test file count: 10\n",
      "\n",
      "19 GakuenNoise\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "20 GarakutayaManta\n",
      "train file count: 77\n",
      "val file count: 15\n",
      "test file count: 11\n",
      "\n",
      "21 GinNoChimera\n",
      "train file count: 77\n",
      "val file count: 15\n",
      "test file count: 11\n",
      "\n",
      "22 Hamlet\n",
      "train file count: 138\n",
      "val file count: 27\n",
      "test file count: 20\n",
      "\n",
      "23 HanzaiKousyouninMinegishiEitarou\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 10\n",
      "\n",
      "24 HaruichibanNoFukukoro\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "25 HarukaRefrain\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "26 HealingPlanet\n",
      "train file count: 71\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "27 HeiseiJimen\n",
      "train file count: 83\n",
      "val file count: 16\n",
      "test file count: 12\n",
      "\n",
      "28 HinagikuKenzan\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "29 HisokaReturns\n",
      "train file count: 67\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "30 JangiriPonpon\n",
      "train file count: 82\n",
      "val file count: 16\n",
      "test file count: 12\n",
      "\n",
      "31 JijiBabaFight\n",
      "train file count: 54\n",
      "val file count: 10\n",
      "test file count: 9\n",
      "\n",
      "32 Joouari\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "33 Jyovolley\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 11\n",
      "\n",
      "34 KarappoHighschool\n",
      "train file count: 90\n",
      "val file count: 18\n",
      "test file count: 13\n",
      "\n",
      "35 KimiHaBokuNoTaiyouDa\n",
      "train file count: 83\n",
      "val file count: 16\n",
      "test file count: 12\n",
      "\n",
      "36 KoukouNoHitotachi\n",
      "train file count: 50\n",
      "val file count: 10\n",
      "test file count: 7\n",
      "\n",
      "37 KuroidoGanka\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 11\n",
      "\n",
      "38 KyokugenCyclone\n",
      "train file count: 43\n",
      "val file count: 8\n",
      "test file count: 7\n",
      "\n",
      "39 LancelotFullThrottle\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "40 MadouTaiga\n",
      "train file count: 63\n",
      "val file count: 12\n",
      "test file count: 9\n",
      "\n",
      "41 MAD_STONE\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "42 MagicianLoad\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 9\n",
      "\n",
      "43 MagicStarGakuin\n",
      "train file count: 61\n",
      "val file count: 12\n",
      "test file count: 9\n",
      "\n",
      "44 MariaSamaNihaNaisyo\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "45 MayaNoAkaiKutsu\n",
      "train file count: 67\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "46 MemorySeijin\n",
      "train file count: 76\n",
      "val file count: 15\n",
      "test file count: 11\n",
      "\n",
      "47 MeteoSanStrikeDesu\n",
      "train file count: 85\n",
      "val file count: 17\n",
      "test file count: 12\n",
      "\n",
      "48 MiraiSan\n",
      "train file count: 80\n",
      "val file count: 16\n",
      "test file count: 11\n",
      "\n",
      "49 MisutenaideDaisy\n",
      "train file count: 83\n",
      "val file count: 16\n",
      "test file count: 12\n",
      "\n",
      "50 MomoyamaHaikagura\n",
      "train file count: 38\n",
      "val file count: 7\n",
      "test file count: 6\n",
      "\n",
      "51 MukoukizuNoChonbo\n",
      "train file count: 47\n",
      "val file count: 9\n",
      "test file count: 7\n",
      "\n",
      "52 MutekiBoukenSyakuma\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 11\n",
      "\n",
      "53 Nekodama\n",
      "train file count: 87\n",
      "val file count: 17\n",
      "test file count: 13\n",
      "\n",
      "54 NichijouSoup\n",
      "train file count: 76\n",
      "val file count: 15\n",
      "test file count: 11\n",
      "\n",
      "55 Ningyoushi\n",
      "train file count: 71\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "56 OhWareraRettouSeitokai\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "57 OL_Lunch\n",
      "train file count: 50\n",
      "val file count: 10\n",
      "test file count: 7\n",
      "\n",
      "58 ParaisoRoad\n",
      "train file count: 78\n",
      "val file count: 15\n",
      "test file count: 12\n",
      "\n",
      "59 PikaruGenkiDesu\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 11\n",
      "\n",
      "60 PLANET7\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "61 PlatinumJungle\n",
      "train file count: 64\n",
      "val file count: 12\n",
      "test file count: 10\n",
      "\n",
      "62 PrayerHaNemurenai\n",
      "train file count: 73\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "63 PrismHeart\n",
      "train file count: 69\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "64 PsychoStaff\n",
      "train file count: 68\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "65 Raphael\n",
      "train file count: 87\n",
      "val file count: 17\n",
      "test file count: 13\n",
      "\n",
      "66 ReveryEarth\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 10\n",
      "\n",
      "67 RinToSiteSippuNoNaka\n",
      "train file count: 114\n",
      "val file count: 22\n",
      "test file count: 16\n",
      "\n",
      "68 RisingGirl\n",
      "train file count: 90\n",
      "val file count: 18\n",
      "test file count: 13\n",
      "\n",
      "69 Saisoku\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 10\n",
      "\n",
      "70 SamayoeruSyonenNiJunaiWo\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "71 SeisinkiVulnus\n",
      "train file count: 75\n",
      "val file count: 15\n",
      "test file count: 10\n",
      "\n",
      "72 SonokiDeABC\n",
      "train file count: 74\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "73 SyabondamaKieta\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "74 TaiyouNiSmash\n",
      "train file count: 68\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "75 TapkunNoTanteisitsu\n",
      "train file count: 80\n",
      "val file count: 16\n",
      "test file count: 11\n",
      "\n",
      "76 TasogareTsushin\n",
      "train file count: 73\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "77 TennenSenshiG\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "78 TensiNoHaneToAkumaNoShippo\n",
      "train file count: 68\n",
      "val file count: 13\n",
      "test file count: 10\n",
      "\n",
      "79 TetsuSan\n",
      "train file count: 30\n",
      "val file count: 6\n",
      "test file count: 4\n",
      "\n",
      "80 That'sIzumiko\n",
      "train file count: 95\n",
      "val file count: 19\n",
      "test file count: 13\n",
      "\n",
      "81 TotteokiNoABC\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "82 ToutaMairimasu\n",
      "train file count: 81\n",
      "val file count: 16\n",
      "test file count: 12\n",
      "\n",
      "83 TouyouKidan\n",
      "train file count: 99\n",
      "val file count: 19\n",
      "test file count: 15\n",
      "\n",
      "84 TsubasaNoKioku\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n",
      "85 UchiNoNyan'sDiary\n",
      "train file count: 61\n",
      "val file count: 12\n",
      "test file count: 9\n",
      "\n",
      "86 UchuKigekiM774\n",
      "train file count: 62\n",
      "val file count: 12\n",
      "test file count: 9\n",
      "\n",
      "87 UltraEleven\n",
      "train file count: 84\n",
      "val file count: 16\n",
      "test file count: 13\n",
      "\n",
      "88 UnbalanceTokyo\n",
      "train file count: 66\n",
      "val file count: 13\n",
      "test file count: 9\n",
      "\n",
      "89 WarewareHaOniDearu\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "90 YamatoNoHane\n",
      "train file count: 85\n",
      "val file count: 17\n",
      "test file count: 12\n",
      "\n",
      "91 YasasiiAkuma\n",
      "train file count: 70\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "92 YouchienBoueigumi\n",
      "train file count: 20\n",
      "val file count: 4\n",
      "test file count: 3\n",
      "\n",
      "93 YoumaKourin\n",
      "train file count: 80\n",
      "val file count: 16\n",
      "test file count: 11\n",
      "\n",
      "94 YukiNoFuruMachi\n",
      "train file count: 71\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "95 YumeiroCooking\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 10\n",
      "\n",
      "96 YumeNoKayoiji\n",
      "train file count: 72\n",
      "val file count: 14\n",
      "test file count: 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(image_root, train_ratio, val_ratio):\n",
    "    for index, manga_title in enumerate(os.listdir(image_root)):\n",
    "        print(index, manga_title)\n",
    "        manga_path = os.path.join(image_root, manga_title)\n",
    "        \n",
    "        if not os.path.isdir(manga_path):\n",
    "            continue\n",
    "\n",
    "        # get all image files in the manga title folder\n",
    "        image_files = sorted([f for f in os.listdir(manga_path) if f.endswith('.jpg')])\n",
    "        # print(image_files)\n",
    "\n",
    "        # shuffle images to ensure random splits\n",
    "        random.shuffle(image_files)\n",
    "        # print(image_files)\n",
    "\n",
    "        # calculate the number of images for each set\n",
    "        total_images = len(image_files)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        val_count = int(total_images * val_ratio)\n",
    "        # print(total_images)\n",
    "        # print(train_count)\n",
    "        # print(val_count)\n",
    "\n",
    "        # split images into train, val, and test\n",
    "        train_files = image_files[:train_count]\n",
    "        val_files = image_files[train_count:train_count + val_count]\n",
    "        test_files = image_files[train_count + val_count:]\n",
    "        # print(train_files)\n",
    "        # print(val_files)\n",
    "        # print(test_files)\n",
    "        # print()\n",
    "\n",
    "        train_page_num = [int(str(num)[:-4]) for num in train_files]\n",
    "        val_page_num = [int(str(num)[:-4]) for num in val_files]\n",
    "        test_page_num = [int(str(num)[:-4]) for num in test_files]\n",
    "        # print(train_page_num)\n",
    "        # print(val_page_num)\n",
    "        # print(test_page_num)\n",
    "        # print()\n",
    "\n",
    "        move_files(manga_path, train_dir, train_files, manga_title, index, train_page_num)\n",
    "        print(f\"train file count: {train_count}\")\n",
    "\n",
    "        move_files(manga_path, val_dir, val_files, manga_title, index, val_page_num)\n",
    "        print(f\"val file count: {val_count}\")\n",
    "\n",
    "        move_files(manga_path, test_dir, test_files, manga_title, index, test_page_num)\n",
    "        print(f\"test file count: {total_images - train_count - val_count}\")\n",
    "\n",
    "        # print(f\"Processed {manga_title}: {train_count} train, {val_count} val, {len(test_files)} test files.\")\n",
    "        print()\n",
    "\n",
    "def move_files(src_folder, dest_folder, files, manga_title, _index, page_num):\n",
    "    for i, file in enumerate(files):\n",
    "        src_path = os.path.join(src_folder, file)\n",
    "        \n",
    "        pg_num = str(page_num[i]).zfill(5)\n",
    "        unique_id = str(_index) + pg_num[3:]\n",
    "        padded_unique_id = unique_id.zfill(5)\n",
    "        # print(padded_unique_id)\n",
    "        # print(page_num)\n",
    "        # print(str(page_num))\n",
    "\n",
    "        if page_num[i] > 99:\n",
    "            unique_id2 = str(_index) + str(page_num[i])\n",
    "            padded_unique_id2 = unique_id2.zfill(5)\n",
    "            # print(padded_unique_id2)\n",
    "            \n",
    "            new_filename = f\"{manga_title}_{padded_unique_id2}.jpg\"\n",
    "            dest_path = os.path.join(dest_folder, new_filename)\n",
    "            # print(new_filename)\n",
    "        else:\n",
    "            # print(padded_unique_id)\n",
    "            new_filename = f\"{manga_title}_{padded_unique_id}.jpg\"\n",
    "            dest_path = os.path.join(dest_folder, new_filename)\n",
    "            # print(new_filename)\n",
    "\n",
    "        # print(new_filename)\n",
    "\n",
    "        shutil.copyfile(src_path, dest_path)\n",
    "        # page_num += 1\n",
    "    # return page_num\n",
    "\n",
    "output_root = '../Manga109/dataset_split/'\n",
    "\n",
    "train_dir = os.path.join(output_root, 'train')\n",
    "val_dir = os.path.join(output_root, 'val')\n",
    "test_dir = os.path.join(output_root, 'test')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "image_root = '../Manga109/images/'\n",
    "train_ratio = 0.75\n",
    "val_ratio = 0.15  \n",
    "\n",
    "split_dataset(image_root, train_ratio, val_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert Manga109 annotations to COCO format for DETR usage\n",
    "- delete image folders & xml files with '_vol' format for ease of annotation\n",
    "- HighschoolKimengumi\n",
    "- LoveHina\n",
    "- MoeruOnisan\n",
    "- SaladDays\n",
    "- ShimatteIkouze\n",
    "\n",
    "#### as well as manga containing numbers, due to annotations error where page unique id is inserdet at '3'\n",
    "- GOOD_KISS_Ver2\n",
    "- Count3DeKimeteAgeru\n",
    "\n",
    "#### which leaves us to Manga097\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished converting xml to json format for train dataset\n",
      "Saved annotation.json file at train folder\n",
      "Finished converting xml to json format for val dataset\n",
      "Saved annotation.json file at val folder\n",
      "Finished converting xml to json format for test dataset\n",
      "Saved annotation.json file at test folder\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image  \n",
    "import re\n",
    "\n",
    "def get_photo_creation_date(file_path):\n",
    "    creation_time = os.path.getctime(file_path)\n",
    "    creation_datetime = datetime.fromtimestamp(creation_time)\n",
    "    \n",
    "    # format \"YYYY-MM-DD HH:MM:SS\"\n",
    "    formatted_date = creation_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "def convert_bbox_to_coco_format(x1, y1, x2, y2):\n",
    "    x = x1\n",
    "    y = y1\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    return [x, y, width, height]\n",
    "\n",
    "def gather_images(manga_title, split_dir, xml_root, filtered_books):\n",
    "    # print(manga_title)\n",
    "    # print('in gather_images()')\n",
    "\n",
    "    images = []\n",
    "    # page_file_list = os.listdir(split_dir)\n",
    "\n",
    "    # filtered_files = [file for file in page_file_list if file.startswith(manga_title + '_')]\n",
    "    # ['AisazuNihaIrarenai_00071.jpg', 'AisazuNihaIrarenai_00072.jpg', 'AisazuNihaIrarenai_00073.jpg', 'AisazuNihaIrarenai_00074.jpg', 'AisazuNihaIrarenai_00075.jpg', 'AisazuNihaIrarenai_00076.jpg', 'AisazuNihaIrarenai_00077.jpg', 'AisazuNihaIrarenai_00078.jpg', 'AisazuNihaIrarenai_00079.jpg', 'AisazuNihaIrarenai_00080.jpg', 'AisazuNihaIrarenai_00081.jpg', 'AisazuNihaIrarenai_00082.jpg', 'AisazuNihaIrarenai_00083.jpg', 'AisazuNihaIrarenai_00084.jpg']\n",
    "    # print(filtered_files)\n",
    "    filtered_files = [file for file in filtered_books if file.startswith(manga_title + '_')]\n",
    "    # print(filtered_files)\n",
    "    # print(len(filtered_files))\n",
    "    # print()\n",
    "\n",
    "    page_list = [page.replace('.jpg', '') for page in filtered_files]\n",
    "    # ['AisazuNihaIrarenai_00071', 'AisazuNihaIrarenai_00072', 'AisazuNihaIrarenai_00073', 'AisazuNihaIrarenai_00074', 'AisazuNihaIrarenai_00075', 'AisazuNihaIrarenai_00076', 'AisazuNihaIrarenai_00077', 'AisazuNihaIrarenai_00078', 'AisazuNihaIrarenai_00079', 'AisazuNihaIrarenai_00080', 'AisazuNihaIrarenai_00081', 'AisazuNihaIrarenai_00082', 'AisazuNihaIrarenai_00083', 'AisazuNihaIrarenai_00084']\n",
    "    # print(page_list)\n",
    "\n",
    "    page_id_str = [page.replace(str(manga_title + '_'), '') for page in page_list]\n",
    "    # ['10578', '10579', '10580', '10581', '10582', '10583', '10584', '10585', '10586', '10587', '10588', '10589', '10590', '10591']\n",
    "    # print(page_id_str)\n",
    "    # print(len(page_id_str))\n",
    "    # print()\n",
    "\n",
    "    unique_page_list_2digits = [str(page)[-2:] for page in page_id_str]\n",
    "    unique_page_list_2digits_int = [int(num) for num in unique_page_list_2digits]\n",
    "    # print('unique_page_list_2digits_int')\n",
    "    # print(unique_page_list_2digits_int)\n",
    "    # print(len(unique_page_list_2digits_int))\n",
    "    # print()\n",
    "\n",
    "    # pad the list unique_page_list_2digits_int\n",
    "    unique_page_list_2digits_int_padded = []\n",
    "    previous_num = unique_page_list_2digits_int[0]\n",
    "    for num in unique_page_list_2digits_int:\n",
    "        if num < previous_num:  # If the current number is less than the previous one, add 100\n",
    "            num += 100\n",
    "        unique_page_list_2digits_int_padded.append(num)\n",
    "        previous_num = num  # Update previous_num to current\n",
    "    \n",
    "    # go through each image annotations\n",
    "    page_split_counter = 0\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "        page_index_int = int(page_index)\n",
    "        # print(page_index_int)\n",
    "\n",
    "        if int(page_index) in unique_page_list_2digits_int_padded:\n",
    "            # print(int(page_index), unique_page_list_2digits_int_padded[page_split_counter])\n",
    "            # print(page_index, unique_page_list_2digits[0])\n",
    "            split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_id_str[page_split_counter]}.jpg\")\n",
    "            images.append({\n",
    "                \"id\": int(page_index),\n",
    "                \"width\": 1654,\n",
    "                \"height\": 1170,\n",
    "                \"file_name\": f\"{manga_title}_{page_index}.jpg\",\n",
    "                # \"date_captured\": ''\n",
    "                \"date_captured\": get_photo_creation_date(split_img_path)\n",
    "            })\n",
    "            page_split_counter += 1       \n",
    "            # print(images[-1:])\n",
    "\n",
    "    # fix numbering on the file name\n",
    "    page_id_str_counter = 0\n",
    "    for image in images:\n",
    "        detected_num_in_file_name = re.search(r'(\\d+)', image['file_name']).group(0)\n",
    "        split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_id_str[page_id_str_counter]}.jpg\")\n",
    "        # print(split_img_path)\n",
    "\n",
    "        image['id'] = int(page_id_str[page_id_str_counter])\n",
    "        image['file_name'] = image['file_name'].replace(detected_num_in_file_name, page_id_str[page_id_str_counter])\n",
    "        image['date_captured'] = get_photo_creation_date(split_img_path)\n",
    "\n",
    "        # print(image)\n",
    "        page_id_str_counter += 1\n",
    "\n",
    "    # print(f'Images annotated:', len(images))\n",
    "    return images\n",
    "\n",
    "def gather_annotations(manga_title, split_dir, xml_root, filtered_books):\n",
    "    annotations = []\n",
    "\n",
    "    filtered_files = [file for file in filtered_books if file.startswith(manga_title + '_')]\n",
    "    page_list = [page.replace('.jpg', '') for page in filtered_files]\n",
    "    page_id_str = [page.replace(str(manga_title + '_'), '') for page in page_list]\n",
    "\n",
    "    # convert the strings into integers\n",
    "    page_id_int = [int(page) for page in page_id_str]\n",
    "    # then sort for proper ordering\n",
    "    sorted_page_id_int = sorted(page_id_int)\n",
    "    sorted_page_id_str = [str(page).zfill(5) for page in sorted_page_id_int]  # Fix indexing issues\n",
    "\n",
    "    # create 2-digit suffixes for the sorted page IDs\n",
    "    unique_page_list_2digits = [str(page)[-2:] for page in sorted_page_id_str]\n",
    "    unique_page_list_2digits_int = [int(num) for num in unique_page_list_2digits]\n",
    "\n",
    "    # adjust the 2-digit list to handle rollovers\n",
    "    unique_page_list_2digits_int_padded = []\n",
    "    previous_num = unique_page_list_2digits_int[0]\n",
    "    for num in unique_page_list_2digits_int:\n",
    "        if num < previous_num:  # If the current number is less than the previous one, add 100\n",
    "            num += 100\n",
    "        unique_page_list_2digits_int_padded.append(num)\n",
    "        previous_num = num\n",
    "\n",
    "    # create mapping from page_index_int to sorted page_id_str\n",
    "    page_index_to_id_str = dict(zip(unique_page_list_2digits_int_padded, sorted_page_id_str))\n",
    "\n",
    "    objects = ['face', 'body', 'text', 'frame']\n",
    "\n",
    "    # variables to keep track of annotations\n",
    "    element_holder = []\n",
    "    page_split_counter = 0\n",
    "    total_elements = 0\n",
    "    element_split_counter = 0\n",
    "\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "        page_index_int = int(page_index)\n",
    "        total_elements += len(page)\n",
    "\n",
    "        # use the padded unique page indices for consistent mapping\n",
    "        if page_index_int in page_index_to_id_str:\n",
    "            # print(f'index: {page_index_int}\\telement count: {len(page)}')\n",
    "            # correctly map to the string page ID using the dictionary\n",
    "            correct_page_id = page_index_to_id_str[page_index_int]\n",
    "            \n",
    "            for element in page:\n",
    "                element_holder.append(page_index)\n",
    "                bbox = element.attrib\n",
    "                category = element.tag\n",
    "\n",
    "                width = int(bbox['xmax']) - int(bbox['xmin'])\n",
    "                height = int(bbox['ymax']) - int(bbox['ymin'])\n",
    "                area = width * height\n",
    "                \n",
    "                annotations.append({\n",
    "                    \"id\": bbox['id'],\n",
    "                    \"image_id\": int(correct_page_id), \n",
    "                    \"category_id\": objects.index(category),\n",
    "                    \"area\": area,\n",
    "                    \"bbox\": convert_bbox_to_coco_format(\n",
    "                        int(bbox['xmin']), int(bbox['ymin']),\n",
    "                        int(bbox['xmax']), int(bbox['ymax'])\n",
    "                    )\n",
    "                })\n",
    "                # print('\\t', correct_page_id)\n",
    "                # print(annotations[-1:])\n",
    "                element_split_counter += 1\n",
    "\n",
    "        page_split_counter += 1\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def gather_categories():\n",
    "    return {\n",
    "        \"categories\": [\n",
    "            {\"id\": 0, \"name\": 'face'},\n",
    "            {\"id\": 1, \"name\": 'body'},\n",
    "            {\"id\": 2, \"name\": 'text'},\n",
    "            {\"id\": 3, \"name\": 'frame'},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def save_coco_annotations(images, annotations, categories_data, coco_json_destination):\n",
    "    coco_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        **categories_data  \n",
    "    }\n",
    "    with open(coco_json_destination, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "def create_json_for_splits(fp_split_dataset):\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "    # for split in ['train']:\n",
    "        image_field = []\n",
    "        annotations_field = []\n",
    "        categories_field = gather_categories()  \n",
    "        \n",
    "        for manga_title in os.listdir('../Manga109/images'):\n",
    "        # for manga_title in ['YumeNoKayoiji']:\n",
    "            # print('in create_json_for_splits()')\n",
    "            # print(manga_title)\n",
    "\n",
    "            split_dir = os.path.join(fp_split_dataset, split)\n",
    "            # print(split)\n",
    "            # print(os.listdir(split_dir))\n",
    "            # print(len(os.listdir(split_dir)))\n",
    "\n",
    "            filtered_books = [title for title in os.listdir(split_dir) if manga_title in title]\n",
    "            # print(filtered_books)\n",
    "            # print(len(filtered_books))\n",
    "            # print()\n",
    "\n",
    "            xml_path = f'../Manga109/annotations/{manga_title}.xml'\n",
    "\n",
    "            if not os.path.exists(xml_path):\n",
    "                print(f'Warning: {xml_path} does not exist. Skipping this manga title.')\n",
    "                continue\n",
    "\n",
    "            tree = ET.parse(xml_path)\n",
    "            xml_root = tree.getroot()\n",
    "\n",
    "            # print(f'Annotating {manga_title}')\n",
    "            images = gather_images(manga_title, split_dir, xml_root, filtered_books)\n",
    "            annotations = gather_annotations(manga_title, split_dir, xml_root, filtered_books)\n",
    "\n",
    "            image_field += images\n",
    "            annotations_field += annotations\n",
    "\n",
    "            coco_json_destination = os.path.join(split_dir, 'annotations.json')  \n",
    "            # print('=======================================')\n",
    "        \n",
    "        print(f'Finished converting xml to json format for {split} dataset')\n",
    "        save_coco_annotations(image_field, annotations_field, categories_field, coco_json_destination)\n",
    "        print(f'Saved annotation.json file at {split} folder')\n",
    "\n",
    "fp_split_dataset = '../Manga109/dataset_split/'\n",
    "create_json_for_splits(fp_split_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
