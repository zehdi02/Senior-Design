{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split dataset to train/val/test folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(image_root, train_ratio, val_ratio):\n",
    "    for index, manga_title in enumerate(os.listdir(image_root)):\n",
    "        print(index, manga_title)\n",
    "        manga_path = os.path.join(image_root, manga_title)\n",
    "        \n",
    "        if not os.path.isdir(manga_path):\n",
    "            continue\n",
    "\n",
    "        # get all image files in the manga title folder\n",
    "        image_files = sorted([f for f in os.listdir(manga_path) if f.endswith('.jpg')])\n",
    "        # print(image_files)\n",
    "\n",
    "        # shuffle images to ensure random splits\n",
    "        random.shuffle(image_files)\n",
    "        # print(image_files)\n",
    "\n",
    "        # calculate the number of images for each set\n",
    "        total_images = len(image_files)\n",
    "        train_count = int(total_images * train_ratio)\n",
    "        val_count = int(total_images * val_ratio)\n",
    "        # print(total_images)\n",
    "        # print(train_count)\n",
    "        # print(val_count)\n",
    "\n",
    "        # split images into train, val, and test\n",
    "        train_files = image_files[:train_count]\n",
    "        val_files = image_files[train_count:train_count + val_count]\n",
    "        test_files = image_files[train_count + val_count:]\n",
    "        # print(train_files)\n",
    "        # print(val_files)\n",
    "        # print(test_files)\n",
    "        # print()\n",
    "\n",
    "        train_page_num = [int(str(num)[:-4]) for num in train_files]\n",
    "        val_page_num = [int(str(num)[:-4]) for num in val_files]\n",
    "        test_page_num = [int(str(num)[:-4]) for num in test_files]\n",
    "        # print(train_page_num)\n",
    "        # print(val_page_num)\n",
    "        # print(test_page_num)\n",
    "        # print()\n",
    "\n",
    "        move_files(manga_path, train_dir, train_files, manga_title, index, train_page_num)\n",
    "        print(f\"train file count: {train_count}\")\n",
    "\n",
    "        move_files(manga_path, val_dir, val_files, manga_title, index, val_page_num)\n",
    "        print(f\"val file count: {val_count}\")\n",
    "\n",
    "        move_files(manga_path, test_dir, test_files, manga_title, index, test_page_num)\n",
    "        print(f\"test file count: {total_images - train_count - val_count}\")\n",
    "\n",
    "        # print(f\"Processed {manga_title}: {train_count} train, {val_count} val, {len(test_files)} test files.\")\n",
    "        print()\n",
    "\n",
    "def move_files(src_folder, dest_folder, files, manga_title, _index, page_num):\n",
    "    for i, file in enumerate(files):\n",
    "        src_path = os.path.join(src_folder, file)\n",
    "        \n",
    "        pg_num = str(page_num[i]).zfill(5)\n",
    "        unique_id = str(_index) + pg_num[3:]\n",
    "        padded_unique_id = unique_id.zfill(5)\n",
    "        # print(padded_unique_id)\n",
    "        # print(page_num)\n",
    "        # print(str(page_num))\n",
    "\n",
    "        if page_num[i] > 99:\n",
    "            unique_id2 = str(_index) + str(page_num[i])\n",
    "            padded_unique_id2 = unique_id2.zfill(5)\n",
    "            # print(padded_unique_id2)\n",
    "            \n",
    "            new_filename = f\"{manga_title}_{padded_unique_id2}.jpg\"\n",
    "            dest_path = os.path.join(dest_folder, new_filename)\n",
    "            # print(new_filename)\n",
    "        else:\n",
    "            # print(padded_unique_id)\n",
    "            new_filename = f\"{manga_title}_{padded_unique_id}.jpg\"\n",
    "            dest_path = os.path.join(dest_folder, new_filename)\n",
    "            # print(new_filename)\n",
    "\n",
    "        # print(new_filename)\n",
    "\n",
    "        shutil.copyfile(src_path, dest_path)\n",
    "        # page_num += 1\n",
    "    # return page_num\n",
    "\n",
    "output_root = '../Manga109/dataset_split/'\n",
    "\n",
    "train_dir = os.path.join(output_root, 'train')\n",
    "val_dir = os.path.join(output_root, 'val')\n",
    "test_dir = os.path.join(output_root, 'test')\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "image_root = '../Manga109/images/'\n",
    "train_ratio = 0.75\n",
    "val_ratio = 0.15  \n",
    "\n",
    "split_dataset(image_root, train_ratio, val_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert Manga109 annotations to COCO format for DETR usage\n",
    "- delete image folders & xml files with '_vol' format for ease of annotation\n",
    "- HighschoolKimengumi\n",
    "- LoveHina\n",
    "- MoeruOnisan\n",
    "- SaladDays\n",
    "- ShimatteIkouze\n",
    "\n",
    "#### as well as manga containing numbers, due to annotations error where page unique id is inserdet at '3'\n",
    "- GOOD_KISS_Ver2\n",
    "- Count3DeKimeteAgeru\n",
    "- PLANET7\n",
    "- UchuKigekiM774\n",
    "\n",
    "#### which leaves us to Manga095\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from PIL import Image  \n",
    "import re\n",
    "\n",
    "def get_photo_creation_date(file_path):\n",
    "    creation_time = os.path.getctime(file_path)\n",
    "    creation_datetime = datetime.fromtimestamp(creation_time)\n",
    "    \n",
    "    # format \"YYYY-MM-DD HH:MM:SS\"\n",
    "    formatted_date = creation_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    return formatted_date\n",
    "\n",
    "def convert_bbox_to_coco_format(x1, y1, x2, y2):\n",
    "    x = x1\n",
    "    y = y1\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    return [x, y, width, height]\n",
    "\n",
    "def gather_images(manga_title, split_dir, xml_root, filtered_books):\n",
    "    # print(manga_title)\n",
    "    # print('in gather_images()')\n",
    "\n",
    "    images = []\n",
    "    # page_file_list = os.listdir(split_dir)\n",
    "\n",
    "    # filtered_files = [file for file in page_file_list if file.startswith(manga_title + '_')]\n",
    "    # ['AisazuNihaIrarenai_00071.jpg', 'AisazuNihaIrarenai_00072.jpg', 'AisazuNihaIrarenai_00073.jpg', 'AisazuNihaIrarenai_00074.jpg', 'AisazuNihaIrarenai_00075.jpg', 'AisazuNihaIrarenai_00076.jpg', 'AisazuNihaIrarenai_00077.jpg', 'AisazuNihaIrarenai_00078.jpg', 'AisazuNihaIrarenai_00079.jpg', 'AisazuNihaIrarenai_00080.jpg', 'AisazuNihaIrarenai_00081.jpg', 'AisazuNihaIrarenai_00082.jpg', 'AisazuNihaIrarenai_00083.jpg', 'AisazuNihaIrarenai_00084.jpg']\n",
    "    # print(filtered_files)\n",
    "    filtered_files = [file for file in filtered_books if file.startswith(manga_title + '_')]\n",
    "    # print(filtered_files)\n",
    "    # print(len(filtered_files))\n",
    "    # print()\n",
    "\n",
    "    page_list = [page.replace('.jpg', '') for page in filtered_files]\n",
    "    # ['AisazuNihaIrarenai_00071', 'AisazuNihaIrarenai_00072', 'AisazuNihaIrarenai_00073', 'AisazuNihaIrarenai_00074', 'AisazuNihaIrarenai_00075', 'AisazuNihaIrarenai_00076', 'AisazuNihaIrarenai_00077', 'AisazuNihaIrarenai_00078', 'AisazuNihaIrarenai_00079', 'AisazuNihaIrarenai_00080', 'AisazuNihaIrarenai_00081', 'AisazuNihaIrarenai_00082', 'AisazuNihaIrarenai_00083', 'AisazuNihaIrarenai_00084']\n",
    "    # print(page_list)\n",
    "\n",
    "    page_id_str = [page.replace(str(manga_title + '_'), '') for page in page_list]\n",
    "    # ['10578', '10579', '10580', '10581', '10582', '10583', '10584', '10585', '10586', '10587', '10588', '10589', '10590', '10591']\n",
    "    # print(page_id_str)\n",
    "    # print(len(page_id_str))\n",
    "    # print()\n",
    "\n",
    "    unique_page_list_2digits = [str(page)[-2:] for page in page_id_str]\n",
    "    unique_page_list_2digits_int = [int(num) for num in unique_page_list_2digits]\n",
    "    # print('unique_page_list_2digits_int')\n",
    "    # print(unique_page_list_2digits_int)\n",
    "    # print(len(unique_page_list_2digits_int))\n",
    "    # print()\n",
    "\n",
    "    # pad the list unique_page_list_2digits_int\n",
    "    unique_page_list_2digits_int_padded = []\n",
    "    previous_num = unique_page_list_2digits_int[0]\n",
    "    for num in unique_page_list_2digits_int:\n",
    "        if num < previous_num:  # If the current number is less than the previous one, add 100\n",
    "            num += 100\n",
    "        unique_page_list_2digits_int_padded.append(num)\n",
    "        previous_num = num  # Update previous_num to current\n",
    "    \n",
    "    # go through each image annotations\n",
    "    page_split_counter = 0\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "        page_index_int = int(page_index)\n",
    "        # print(page_index_int)\n",
    "\n",
    "        if int(page_index) in unique_page_list_2digits_int_padded:\n",
    "            # print(int(page_index), unique_page_list_2digits_int_padded[page_split_counter])\n",
    "            # print(page_index, unique_page_list_2digits[0])\n",
    "            split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_id_str[page_split_counter]}.jpg\")\n",
    "            images.append({\n",
    "                \"id\": int(page_index),\n",
    "                \"width\": 1654,\n",
    "                \"height\": 1170,\n",
    "                \"file_name\": f\"{manga_title}_{page_index}.jpg\",\n",
    "                # \"date_captured\": ''\n",
    "                \"date_captured\": get_photo_creation_date(split_img_path)\n",
    "            })\n",
    "            page_split_counter += 1       \n",
    "            # print(images[-1:])\n",
    "\n",
    "    # fix numbering on the file name\n",
    "    page_id_str_counter = 0\n",
    "    for image in images:\n",
    "        detected_num_in_file_name = re.search(r'(\\d+)', image['file_name']).group(0)\n",
    "        split_img_path = os.path.join(split_dir, f\"{manga_title}_{page_id_str[page_id_str_counter]}.jpg\")\n",
    "        # print(split_img_path)\n",
    "\n",
    "        image['id'] = int(page_id_str[page_id_str_counter])\n",
    "        image['file_name'] = image['file_name'].replace(detected_num_in_file_name, page_id_str[page_id_str_counter])\n",
    "        image['date_captured'] = get_photo_creation_date(split_img_path)\n",
    "\n",
    "        # print(image)\n",
    "        page_id_str_counter += 1\n",
    "\n",
    "    # print(f'Images annotated:', len(images))\n",
    "    return images\n",
    "\n",
    "def gather_annotations(manga_title, split_dir, xml_root, filtered_books):\n",
    "    annotations = []\n",
    "\n",
    "    filtered_files = [file for file in filtered_books if file.startswith(manga_title + '_')]\n",
    "    page_list = [page.replace('.jpg', '') for page in filtered_files]\n",
    "    page_id_str = [page.replace(str(manga_title + '_'), '') for page in page_list]\n",
    "\n",
    "    # convert the strings into integers\n",
    "    page_id_int = [int(page) for page in page_id_str]\n",
    "    # then sort for proper ordering\n",
    "    sorted_page_id_int = sorted(page_id_int)\n",
    "    sorted_page_id_str = [str(page).zfill(5) for page in sorted_page_id_int]  # Fix indexing issues\n",
    "\n",
    "    # create 2-digit suffixes for the sorted page IDs\n",
    "    unique_page_list_2digits = [str(page)[-2:] for page in sorted_page_id_str]\n",
    "    unique_page_list_2digits_int = [int(num) for num in unique_page_list_2digits]\n",
    "\n",
    "    # adjust the 2-digit list to handle rollovers\n",
    "    unique_page_list_2digits_int_padded = []\n",
    "    previous_num = unique_page_list_2digits_int[0]\n",
    "    for num in unique_page_list_2digits_int:\n",
    "        if num < previous_num:  # If the current number is less than the previous one, add 100\n",
    "            num += 100\n",
    "        unique_page_list_2digits_int_padded.append(num)\n",
    "        previous_num = num\n",
    "\n",
    "    # create mapping from page_index_int to sorted page_id_str\n",
    "    page_index_to_id_str = dict(zip(unique_page_list_2digits_int_padded, sorted_page_id_str))\n",
    "\n",
    "    objects = ['face', 'body', 'text', 'frame']\n",
    "\n",
    "    # variables to keep track of annotations\n",
    "    element_holder = []\n",
    "    page_split_counter = 0\n",
    "    total_elements = 0\n",
    "    element_split_counter = 0\n",
    "\n",
    "    for page in xml_root.findall(\".//page\"):\n",
    "        page_index = page.get('index')\n",
    "        page_index_int = int(page_index)\n",
    "        total_elements += len(page)\n",
    "\n",
    "        # use the padded unique page indices for consistent mapping\n",
    "        if page_index_int in page_index_to_id_str:\n",
    "            # print(f'index: {page_index_int}\\telement count: {len(page)}')\n",
    "            # correctly map to the string page ID using the dictionary\n",
    "            correct_page_id = page_index_to_id_str[page_index_int]\n",
    "            \n",
    "            for element in page:\n",
    "                element_holder.append(page_index)\n",
    "                bbox = element.attrib\n",
    "                category = element.tag\n",
    "\n",
    "                width = int(bbox['xmax']) - int(bbox['xmin'])\n",
    "                height = int(bbox['ymax']) - int(bbox['ymin'])\n",
    "                area = width * height\n",
    "                \n",
    "                annotations.append({\n",
    "                    \"id\": bbox['id'],\n",
    "                    \"image_id\": int(correct_page_id), \n",
    "                    \"category_id\": objects.index(category),\n",
    "                    \"area\": area,\n",
    "                    \"bbox\": convert_bbox_to_coco_format(\n",
    "                        int(bbox['xmin']), int(bbox['ymin']),\n",
    "                        int(bbox['xmax']), int(bbox['ymax'])\n",
    "                    )\n",
    "                })\n",
    "                # print('\\t', correct_page_id)\n",
    "                # print(annotations[-1:])\n",
    "                element_split_counter += 1\n",
    "\n",
    "        page_split_counter += 1\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def gather_categories():\n",
    "    return {\n",
    "        \"categories\": [\n",
    "            {\"id\": 0, \"name\": 'face'},\n",
    "            {\"id\": 1, \"name\": 'body'},\n",
    "            {\"id\": 2, \"name\": 'text'},\n",
    "            {\"id\": 3, \"name\": 'frame'},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def save_coco_annotations(images, annotations, categories_data, coco_json_destination):\n",
    "    coco_data = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        **categories_data  \n",
    "    }\n",
    "    with open(coco_json_destination, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "def create_json_for_splits(fp_split_dataset):\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "    # for split in ['train']:\n",
    "        image_field = []\n",
    "        annotations_field = []\n",
    "        categories_field = gather_categories()  \n",
    "        \n",
    "        for manga_title in os.listdir('../Manga109/images'):\n",
    "        # for manga_title in ['YumeNoKayoiji']:\n",
    "            # print('in create_json_for_splits()')\n",
    "            # print(manga_title)\n",
    "\n",
    "            split_dir = os.path.join(fp_split_dataset, split)\n",
    "            # print(split)\n",
    "            # print(os.listdir(split_dir))\n",
    "            # print(len(os.listdir(split_dir)))\n",
    "\n",
    "            filtered_books = [title for title in os.listdir(split_dir) if manga_title in title]\n",
    "            # print(filtered_books)\n",
    "            # print(len(filtered_books))\n",
    "            # print()\n",
    "\n",
    "            xml_path = f'../Manga109/annotations/{manga_title}.xml'\n",
    "\n",
    "            if not os.path.exists(xml_path):\n",
    "                print(f'Warning: {xml_path} does not exist. Skipping this manga title.')\n",
    "                continue\n",
    "\n",
    "            tree = ET.parse(xml_path)\n",
    "            xml_root = tree.getroot()\n",
    "\n",
    "            # print(f'Annotating {manga_title}')\n",
    "            images = gather_images(manga_title, split_dir, xml_root, filtered_books)\n",
    "            annotations = gather_annotations(manga_title, split_dir, xml_root, filtered_books)\n",
    "\n",
    "            image_field += images\n",
    "            annotations_field += annotations\n",
    "\n",
    "            coco_json_destination = os.path.join(split_dir, 'annotations.json')  \n",
    "            # print('=======================================')\n",
    "        \n",
    "        print(f'Finished converting xml to json format for {split} dataset')\n",
    "        save_coco_annotations(image_field, annotations_field, categories_field, coco_json_destination)\n",
    "        print(f'Saved annotation.json file at {split} folder')\n",
    "\n",
    "fp_split_dataset = '../Manga109/dataset_split/'\n",
    "create_json_for_splits(fp_split_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
