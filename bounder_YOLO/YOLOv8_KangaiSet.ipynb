{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\n",
      "\u001b[2K\n",
      "Ultralytics YOLOv8.2.95 🚀 Python-3.12.4 torch-2.4.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 6144MiB)\n",
      "Setup complete ✅ (12 CPUs, 31.9 GB RAM, 361.0/475.6 GB disk)\n",
      "\n",
      "OS                  Windows-11-10.0.22631-SP0\n",
      "Environment         Windows\n",
      "Python              3.12.4\n",
      "Install             pip\n",
      "RAM                 31.91 GB\n",
      "CPU                 Intel Core(TM) i7-10750H 2.60GHz\n",
      "CUDA                11.8\n",
      "\n",
      "numpy               ✅ 1.26.4<2.0.0,>=1.23.0\n",
      "matplotlib          ✅ 3.8.4>=3.3.0\n",
      "opencv-python       ✅ 4.10.0.84>=4.6.0\n",
      "pillow              ✅ 10.2.0>=7.1.2\n",
      "pyyaml              ✅ 6.0.1>=5.3.1\n",
      "requests            ✅ 2.32.2>=2.23.0\n",
      "scipy               ✅ 1.13.1>=1.4.1\n",
      "torch               ✅ 2.4.1+cu118>=1.8.0\n",
      "torchvision         ✅ 0.19.1+cu118>=0.9.0\n",
      "tqdm                ✅ 4.66.4>=4.64.0\n",
      "psutil              ✅ 5.9.0\n",
      "py-cpuinfo          ✅ 9.0.0\n",
      "pandas              ✅ 2.2.2>=1.1.4\n",
      "seaborn             ✅ 0.13.2>=0.11.0\n",
      "ultralytics-thop    ✅ 2.0.6>=2.0.0\n",
      "torch               ✅ 2.4.1+cu118!=2.4.0,>=1.8.0; sys_platform == \"win32\"\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "from IPython.display import display, Image\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "!yolo checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process KangaiSet+Manga109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9350 images left to process\n",
      "9300 images left to process\n",
      "9250 images left to process\n",
      "9200 images left to process\n",
      "9150 images left to process\n",
      "9100 images left to process\n",
      "9050 images left to process\n",
      "9000 images left to process\n",
      "8950 images left to process\n",
      "8900 images left to process\n",
      "8850 images left to process\n",
      "8800 images left to process\n",
      "8750 images left to process\n",
      "8700 images left to process\n",
      "8650 images left to process\n",
      "8600 images left to process\n",
      "8550 images left to process\n",
      "8500 images left to process\n",
      "8450 images left to process\n",
      "8400 images left to process\n",
      "8350 images left to process\n",
      "8300 images left to process\n",
      "8250 images left to process\n",
      "8200 images left to process\n",
      "8150 images left to process\n",
      "8100 images left to process\n",
      "8050 images left to process\n",
      "8000 images left to process\n",
      "7950 images left to process\n",
      "7900 images left to process\n",
      "7850 images left to process\n",
      "7800 images left to process\n",
      "7750 images left to process\n",
      "7700 images left to process\n",
      "7650 images left to process\n",
      "7600 images left to process\n",
      "7550 images left to process\n",
      "7500 images left to process\n",
      "7450 images left to process\n",
      "7400 images left to process\n",
      "7350 images left to process\n",
      "7300 images left to process\n",
      "7250 images left to process\n",
      "7200 images left to process\n",
      "7150 images left to process\n",
      "7100 images left to process\n",
      "7050 images left to process\n",
      "7000 images left to process\n",
      "6950 images left to process\n",
      "6900 images left to process\n",
      "6850 images left to process\n",
      "6800 images left to process\n",
      "6750 images left to process\n",
      "6700 images left to process\n",
      "6650 images left to process\n",
      "6600 images left to process\n",
      "6550 images left to process\n",
      "6500 images left to process\n",
      "6450 images left to process\n",
      "6400 images left to process\n",
      "6350 images left to process\n",
      "6300 images left to process\n",
      "6250 images left to process\n",
      "6200 images left to process\n",
      "6150 images left to process\n",
      "6100 images left to process\n",
      "6050 images left to process\n",
      "6000 images left to process\n",
      "5950 images left to process\n",
      "5900 images left to process\n",
      "5850 images left to process\n",
      "5800 images left to process\n",
      "5750 images left to process\n",
      "5700 images left to process\n",
      "5650 images left to process\n",
      "5600 images left to process\n",
      "5550 images left to process\n",
      "5500 images left to process\n",
      "5450 images left to process\n",
      "5400 images left to process\n",
      "5350 images left to process\n",
      "5300 images left to process\n",
      "5250 images left to process\n",
      "5200 images left to process\n",
      "5150 images left to process\n",
      "5100 images left to process\n",
      "5050 images left to process\n",
      "5000 images left to process\n",
      "4950 images left to process\n",
      "4900 images left to process\n",
      "4850 images left to process\n",
      "4800 images left to process\n",
      "4750 images left to process\n",
      "4700 images left to process\n",
      "4650 images left to process\n",
      "4600 images left to process\n",
      "4550 images left to process\n",
      "4500 images left to process\n",
      "4450 images left to process\n",
      "4400 images left to process\n",
      "4350 images left to process\n",
      "4300 images left to process\n",
      "4250 images left to process\n",
      "4200 images left to process\n",
      "4150 images left to process\n",
      "4100 images left to process\n",
      "4050 images left to process\n",
      "4000 images left to process\n",
      "3950 images left to process\n",
      "3900 images left to process\n",
      "3850 images left to process\n",
      "3800 images left to process\n",
      "3750 images left to process\n",
      "3700 images left to process\n",
      "3650 images left to process\n",
      "3600 images left to process\n",
      "3550 images left to process\n",
      "3500 images left to process\n",
      "3450 images left to process\n",
      "3400 images left to process\n",
      "3350 images left to process\n",
      "3300 images left to process\n",
      "3250 images left to process\n",
      "3200 images left to process\n",
      "3150 images left to process\n",
      "3100 images left to process\n",
      "3050 images left to process\n",
      "3000 images left to process\n",
      "2950 images left to process\n",
      "2900 images left to process\n",
      "2850 images left to process\n",
      "2800 images left to process\n",
      "2750 images left to process\n",
      "2700 images left to process\n",
      "2650 images left to process\n",
      "2600 images left to process\n",
      "2550 images left to process\n",
      "2500 images left to process\n",
      "2450 images left to process\n",
      "2400 images left to process\n",
      "2350 images left to process\n",
      "2300 images left to process\n",
      "2250 images left to process\n",
      "2200 images left to process\n",
      "2150 images left to process\n",
      "2100 images left to process\n",
      "2050 images left to process\n",
      "2000 images left to process\n",
      "1950 images left to process\n",
      "1900 images left to process\n",
      "1850 images left to process\n",
      "1800 images left to process\n",
      "1750 images left to process\n",
      "1700 images left to process\n",
      "1650 images left to process\n",
      "1600 images left to process\n",
      "1550 images left to process\n",
      "1500 images left to process\n",
      "1450 images left to process\n",
      "1400 images left to process\n",
      "1350 images left to process\n",
      "1300 images left to process\n",
      "1250 images left to process\n",
      "1200 images left to process\n",
      "1150 images left to process\n",
      "1100 images left to process\n",
      "1050 images left to process\n",
      "1000 images left to process\n",
      "950 images left to process\n",
      "900 images left to process\n",
      "850 images left to process\n",
      "800 images left to process\n",
      "750 images left to process\n",
      "700 images left to process\n",
      "650 images left to process\n",
      "600 images left to process\n",
      "550 images left to process\n",
      "500 images left to process\n",
      "450 images left to process\n",
      "400 images left to process\n",
      "350 images left to process\n",
      "300 images left to process\n",
      "250 images left to process\n",
      "200 images left to process\n",
      "150 images left to process\n",
      "100 images left to process\n",
      "50 images left to process\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# make train, val, test dirs\n",
    "kangai_yolo_dir = 'KangaiSet_YOLO'\n",
    "for dataset_type in ['train', 'val', 'test']:\n",
    "  os.makedirs(f'../{kangai_yolo_dir}/{dataset_type}/images', exist_ok=True)\n",
    "  os.makedirs(f'../{kangai_yolo_dir}/{dataset_type}/labels', exist_ok=True)\n",
    "\n",
    "width = 1654\n",
    "height = 1170\n",
    "emotion_map = {\n",
    "    'anger': 0,\n",
    "    'disgust': 1,\n",
    "    'fear': 2,\n",
    "    'happiness': 3,\n",
    "    'neutral': 4,\n",
    "    'sadness': 5,\n",
    "    'surprise': 6\n",
    "}\n",
    "\n",
    "root_dir = '../Manga109'\n",
    "images_dir = os.path.join(root_dir, 'images')\n",
    "annotations_dir = os.path.join(root_dir, 'annotations')\n",
    "\n",
    "df = pd.read_csv('../Manga109/kangaiset_annotation_long.csv')\n",
    "df = df.sort_values(by=['page_path']) # sort df by page_path for ease of annotation\n",
    "\n",
    "count_row = df.shape[0]  # Gives number of rows\n",
    "\n",
    "# go through kangaiset_annotation_long.csv\n",
    "for index, row in df.iterrows():\n",
    "  \n",
    "  manga_name = row['manga_name']\n",
    "  page_path = row['page_path']\n",
    "\n",
    "  book_page = page_path.strip('images/')[:-3].replace('/','_')\n",
    "  page_num = book_page[len(book_page)-3:]\n",
    "  # print(book_page, page_num)\n",
    "\n",
    "  # randomly assign to train/val/test and copy image + txt file to new dir\n",
    "  dataset_type = random.choices(['train', 'val', 'test'], weights=[0.85, 0.10, 0.05])[0]\n",
    "\n",
    "  src_image_path = os.path.join(images_dir, f'{manga_name}/{page_num}.jpg')\n",
    "  yolo_image_path = os.path.join(f'../{kangai_yolo_dir}/{dataset_type}/images/{book_page}.jpg')\n",
    "\n",
    "  # copy image from Manga109 to KangaiSet_YOLO\n",
    "  shutil.copyfile(src_image_path, yolo_image_path)\n",
    "\n",
    "  face_xmin = row['face_xmin']\t\n",
    "  face_ymin = row['face_ymin']\t\n",
    "  face_xmax = row['face_xmax']\t\n",
    "  face_ymax = row['face_ymax']\n",
    "\n",
    "  emotion_str = row['emotion_str']\n",
    "  # emotions = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "  label_file_path = f'../{kangai_yolo_dir}/{dataset_type}/labels/{book_page}.txt'\n",
    "  # label_file = open(label_file_path, 'a')\n",
    "  with open(label_file_path, 'a') as label_file:\n",
    "    if face_xmin < face_xmax and face_ymin < face_ymax:\n",
    "      bbox = ((face_xmin + face_xmax) / (2 * width),\n",
    "              (face_ymin + face_ymax) / (2 * height),\n",
    "              (face_xmax - face_xmin) / width,\n",
    "              (face_ymax - face_ymin) / height,\n",
    "              )\n",
    "      # label_file.write(f'{emotions.index(emotion_str)} {\" \".join(map(str, bbox))}\\n')\n",
    "      label_file.write(f'{emotion_map[emotion_str]} {\" \".join(map(str, bbox))}\\n')\n",
    "  # label_file.close()\n",
    "\n",
    "  count_row -= 1\n",
    "  if count_row > 0:\n",
    "    if count_row % 50 == 0:\n",
    "      print(f'{count_row} images left to process')\n",
    "  else:\n",
    "    print('Processing complete!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.96 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.95 🚀 Python-3.12.4 torch-2.4.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=kangaiset.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.5, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\trainer.py\", line 557, in get_dataset\n",
      "    data = check_det_dataset(self.args.data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\ultralytics\\data\\utils.py\", line 278, in check_det_dataset\n",
      "    data = yaml_load(file, append_filename=True)  # dictionary\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\__init__.py\", line 486, in yaml_load\n",
      "    data = yaml.safe_load(s) or {}  # always return a dict (yaml.safe_load() may return None for empty files)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\__init__.py\", line 125, in safe_load\n",
      "    return load(stream, SafeLoader)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\__init__.py\", line 81, in load\n",
      "    return loader.get_single_data()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\constructor.py\", line 49, in get_single_data\n",
      "    node = self.get_single_node()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\composer.py\", line 36, in get_single_node\n",
      "    document = self.compose_document()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\composer.py\", line 55, in compose_document\n",
      "    node = self.compose_node(None, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\composer.py\", line 127, in compose_mapping_node\n",
      "    while not self.check_event(MappingEndEvent):\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\parser.py\", line 98, in check_event\n",
      "    self.current_event = self.state()\n",
      "                         ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\parser.py\", line 428, in parse_block_mapping_key\n",
      "    if self.check_token(KeyToken):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\scanner.py\", line 115, in check_token\n",
      "    while self.need_more_tokens():\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\scanner.py\", line 152, in need_more_tokens\n",
      "    self.stale_possible_simple_keys()\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\yaml\\scanner.py\", line 291, in stale_possible_simple_keys\n",
      "    raise ScannerError(\"while scanning a simple key\", key.mark,\n",
      "yaml.scanner.ScannerError: while scanning a simple key\n",
      "  in \"<unicode string>\", line 7, column 1:\n",
      "    nc:7\n",
      "    ^\n",
      "could not find expected ':'\n",
      "  in \"<unicode string>\", line 8, column 1:\n",
      "    names:\n",
      "    ^\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Zed\\anaconda3\\Scripts\\yolo.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py\", line 830, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py\", line 809, in train\n",
      "    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\trainer.py\", line 133, in __init__\n",
      "    self.trainset, self.testset = self.get_dataset()\n",
      "                                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Zed\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\trainer.py\", line 561, in get_dataset\n",
      "    raise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error ❌ {e}\")) from e\n",
      "RuntimeError: Dataset 'kangaiset.yaml' error  while scanning a simple key\n",
      "  in \"<unicode string>\", line 7, column 1:\n",
      "    nc:7\n",
      "    ^\n",
      "could not find expected ':'\n",
      "  in \"<unicode string>\", line 8, column 1:\n",
      "    names:\n",
      "    ^\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "!yolo task=detect mode=train model=yolov8n.pt data=kangaiset.yaml lr0=0.001 epochs=50 batch=16 device=cuda dropout=0.5 val=True save=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RE-TRAIN\n",
    "!yolo task=detect mode=train model='runs/detect/train2/weights/best.pt' data=kangaiset.yaml epochs=50 batch=16 device=cuda dropout=0.5 val=True save=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.95 🚀 Python-3.12.4 torch-2.4.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 6144MiB)\n",
      "Model summary (fused): 168 layers, 3,007,013 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 c:\\Users\\Zed\\Documents\\Code\\__Senior Design\\Senior-Design\\bounder_YOLO\\Akuhamu_005.jpg: 480x640 (no detections), 26.6ms\n",
      "Speed: 2.5ms preprocess, 26.6ms inference, 18.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict3\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# test predict w/ best.pt\n",
    "!yolo predict model='runs/detect/train2/weights/best.pt' source='test_page.jpeg' device=cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
